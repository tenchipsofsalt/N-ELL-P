{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functools\n",
    "import pandas as pd\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.data import load\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformer import positional_encoding, EncoderLayer\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3911, 8) (3, 2) (3, 7)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"kaggle/input/\"\n",
    "train_df = pd.read_csv(data_dir + 'feedback-prize-english-language-learning/train.csv')\n",
    "test_df = pd.read_csv(data_dir + 'feedback-prize-english-language-learning/test.csv')\n",
    "sample_df = pd.read_csv(data_dir + 'feedback-prize-english-language-learning/sample_submission.csv')\n",
    "print(train_df.shape, test_df.shape, sample_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>The best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>Small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n",
       "3  003885A45F42  The best time in life is when you become yours...       4.5   \n",
       "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  \n",
       "0     3.5         3.0          3.0      4.0          3.0  \n",
       "1     2.5         3.0          2.0      2.0          2.5  \n",
       "2     3.5         3.0          3.0      3.0          2.5  \n",
       "3     4.5         4.5          4.5      4.0          5.0  \n",
       "4     3.0         3.0          3.0      2.5          2.5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3128, 8) (782, 8)\n"
     ]
    }
   ],
   "source": [
    "size = train_df.shape[0]\n",
    "train, validate = int(0.8*size), int(0.2*size)\n",
    "valid_df = train_df.tail(validate).copy()\n",
    "train_df = train_df.head(train).copy()\n",
    "print(train_df.shape, valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "    def __init__(self, init_vocab = set(), truncate=512):\n",
    "        self.text_vocab = init_vocab\n",
    "        self.truncate = truncate\n",
    "        pass\n",
    "\n",
    "    def preprocess(self, text) :\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+','', text)\n",
    "        text = re.sub(r'@[0-9a-zA-Z]*\\W+',' ' , text)\n",
    "\n",
    "        text = re.sub(r'\\d+', ' ', text)\n",
    "        text = re.sub(r'\\#', ' ', text)\n",
    "        text = re.sub(r'\\'', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        list_text = text.split()\n",
    "        self.text_vocab.update(list_text)\n",
    "        text = ' '.join(list_text[:self.truncate])\n",
    "        return text\n",
    "    \n",
    "    def reset_vocab(self):\n",
    "        self.text_vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word', 'count', 'word_freq', 'log_freq']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vocab.word_freqs import WordFreqDataLoader\n",
    "wfdl = WordFreqDataLoader('./datasets/unigram_freq.csv')\n",
    "wfdl.df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "      <th>word_freq</th>\n",
       "      <th>log_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>23135851162</td>\n",
       "      <td>0.039338</td>\n",
       "      <td>0.932970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>13151942776</td>\n",
       "      <td>0.022363</td>\n",
       "      <td>1.497784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>12997637966</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>1.509585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>12136980858</td>\n",
       "      <td>0.020637</td>\n",
       "      <td>1.578096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>9081174698</td>\n",
       "      <td>0.015441</td>\n",
       "      <td>1.868150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word        count  word_freq  log_freq\n",
       "0  the  23135851162   0.039338  0.932970\n",
       "1   of  13151942776   0.022363  1.497784\n",
       "2  and  12997637966   0.022100  1.509585\n",
       "3   to  12136980858   0.020637  1.578096\n",
       "4    a   9081174698   0.015441  1.868150"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wfdl.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "\n",
    "def compute_and_pad(text):\n",
    "    not_padded = wfdl.get_list_logs(text.split())\n",
    "    padded = np.pad(not_padded, (0, preprocessor.truncate - len(not_padded)), 'constant', constant_values=0)\n",
    "    return padded\n",
    "\n",
    "\n",
    "for dataset in [train_df, valid_df, test_df]:\n",
    "    dataset['full_text'] = dataset['full_text'].apply(lambda text: preprocessor.preprocess(text))\n",
    "    dataset['log_freqs'] = dataset['full_text'].apply(lambda text:  compute_and_pad(text)) # np array in each index\n",
    "\n",
    "text_vocab = preprocessor.text_vocab\n",
    "all_data = pd.concat((train_df, valid_df, test_df)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "      <th>log_freqs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>i think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[2.9474044, 5.5982714, 2.8505685, 5.6600685, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>when a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>[4.504181, 1.8681495, 6.033263, 2.5255692, 1.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>dear principal if u change the school policy o...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>[7.956408, 7.651186, 3.947731, 5.7912893, 5.63...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>the best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[0.9329697, 5.0636106, 4.1700873, 1.9378928, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>[5.642784, 6.0328083, 1.4977837, 10.056067, 3....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  i think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  when a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  dear principal if u change the school policy o...       3.0   \n",
       "3  003885A45F42  the best time in life is when you become yours...       4.5   \n",
       "4  0049B1DF5CCC  small act of kindness can impact in other peop...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  \\\n",
       "0     3.5         3.0          3.0      4.0          3.0   \n",
       "1     2.5         3.0          2.0      2.0          2.5   \n",
       "2     3.5         3.0          3.0      3.0          2.5   \n",
       "3     4.5         4.5          4.5      4.0          5.0   \n",
       "4     3.0         3.0          3.0      2.5          2.5   \n",
       "\n",
       "                                           log_freqs  \n",
       "0  [2.9474044, 5.5982714, 2.8505685, 5.6600685, 4...  \n",
       "1  [4.504181, 1.8681495, 6.033263, 2.5255692, 1.8...  \n",
       "2  [7.956408, 7.651186, 3.947731, 5.7912893, 5.63...  \n",
       "3  [0.9329697, 5.0636106, 4.1700873, 1.9378928, 5...  \n",
       "4  [5.642784, 6.0328083, 1.4977837, 10.056067, 3....  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VocabModel(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               max_text_len, text_vocab, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.max_text_len = max_text_len\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.text_vectorization = tf.keras.layers.TextVectorization(output_mode='int', output_sequence_length=max_text_len)\n",
    "            self.text_vectorization.adapt(text_vocab)\n",
    "            self.word_embedding = tf.keras.layers.Embedding(self.text_vectorization.vocabulary_size(), d_model) # replace\n",
    "        \n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.pos_encoding = tf.Variable(positional_encoding(length=max_text_len, depth=d_model), trainable=False)\n",
    "        self.pos_scalar = tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.inter_dense = tf.keras.layers.Dense(d_model, activation='relu')\n",
    "        self.dense = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        - inputs is a tuple of length 2.\n",
    "        first index is the words (batch_size,)\n",
    "        second index is the log freqs (batch_size, num_words)\n",
    "        \"\"\"\n",
    "        words = inputs[0] # (batch_size,)\n",
    "        log_freqs = tf.expand_dims(tf.convert_to_tensor(inputs[1], dtype=tf.float32), -1)\n",
    "        x = self.word_embedding(self.text_vectorization(words))\n",
    "\n",
    "        # positional encoding\n",
    "        x = x * self.pos_scalar\n",
    "        x = x + self.pos_encoding\n",
    "        # dropout\n",
    "        x = self.dropout(x)\n",
    "        # add encoding layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        # should be of shape (batch_size, num_words, embed_size)\n",
    "        # we want to add on a (batch_size, num_words, 1)\n",
    "        x = tf.concat((x, log_freqs), axis=2)\n",
    "        # now have (batch_size, num_words, embed_size + 1)\n",
    "        x = self.inter_dense(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.layernorm(x)\n",
    "        return self.dense(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Column-wise RMSE\n",
    "def MCRMSE(y_true, y_pred):\n",
    "    mcrmse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n",
    "    return tf.reduce_mean(tf.sqrt(mcrmse), axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    num_layers = 2\n",
    "    d_model = 64\n",
    "    dff = 256\n",
    "    num_heads = 8\n",
    "    dropout_rate = 0.1\n",
    "    max_text_len = preprocessor.truncate\n",
    "    model = VocabModel(num_layers, d_model, num_heads, dff, max_text_len, np.array(list(text_vocab)), dropout_rate)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-5, clipnorm=1), loss=MCRMSE, metrics=MCRMSE, run_eagerly=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 11:43:52.202930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-09 11:43:52.226449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-09 11:43:52.226871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-09 11:43:52.227544: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-09 11:43:52.229074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-09 11:43:52.229386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-09 11:43:52.229644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-09 11:43:52.838270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-09 11:43:52.838589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-09 11:43:52.838603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-12-09 11:43:52.838882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-09 11:43:52.838939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2108 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 11:43:55.647714: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.1230109]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model((tf.expand_dims(tf.convert_to_tensor(train_df.iloc[0]['full_text']), 0), tf.expand_dims(tf.convert_to_tensor(train_df.iloc[0]['log_freqs']), 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      " 44/391 [==>...........................] - ETA: 4:02 - loss: 0.8540 - MCRMSE: 0.8540"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m checkpoint_filepath \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcheckpoints/vocab/run_2/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      2\u001b[0m model_checkpoint_callback \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(\n\u001b[1;32m      3\u001b[0m     filepath\u001b[39m=\u001b[39mcheckpoint_filepath,\n\u001b[1;32m      4\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     10\u001b[0m                     (train_df[\u001b[39m'\u001b[39;49m\u001b[39mfull_text\u001b[39;49m\u001b[39m'\u001b[39;49m], np\u001b[39m.\u001b[39;49marray(train_df[\u001b[39m'\u001b[39;49m\u001b[39mlog_freqs\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39;49mtolist())),\n\u001b[1;32m     11\u001b[0m                     train_df[\u001b[39m'\u001b[39;49m\u001b[39mvocabulary\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     12\u001b[0m                     validation_data \u001b[39m=\u001b[39;49m ((valid_df[\u001b[39m'\u001b[39;49m\u001b[39mfull_text\u001b[39;49m\u001b[39m'\u001b[39;49m], np\u001b[39m.\u001b[39;49marray(valid_df[\u001b[39m'\u001b[39;49m\u001b[39mlog_freqs\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39;49mtolist())), valid_df[\u001b[39m'\u001b[39;49m\u001b[39mvocabulary\u001b[39;49m\u001b[39m'\u001b[39;49m]),\n\u001b[1;32m     13\u001b[0m                     steps_per_epoch\u001b[39m=\u001b[39;49m train_df\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]\u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m                     batch_size \u001b[39m=\u001b[39;49m \u001b[39m8\u001b[39;49m,\n\u001b[1;32m     15\u001b[0m                     epochs\u001b[39m=\u001b[39;49m \u001b[39m25\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m                     verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m                     shuffle\u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, callbacks\u001b[39m=\u001b[39;49m[model_checkpoint_callback])\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/keras/engine/training.py:1216\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1210\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1211\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1212\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1213\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1214\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1215\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1216\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1217\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1218\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/keras/engine/training.py:878\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_function\u001b[39m(iterator):\n\u001b[1;32m    877\u001b[0m   \u001b[39m\"\"\"Runs a training execution with one step.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m   \u001b[39mreturn\u001b[39;00m step_function(\u001b[39mself\u001b[39;49m, iterator)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/keras/engine/training.py:867\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m    866\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[0;32m--> 867\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[1;32m    868\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[1;32m    869\u001b[0m     outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    870\u001b[0m write_scalar_summaries(outputs, step\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39m_train_counter)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1316\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope():\n\u001b[1;32m   1312\u001b[0m   \u001b[39m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m   \u001b[39m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m   1315\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 1316\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extended\u001b[39m.\u001b[39;49mcall_for_each_replica(fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2892\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2890\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m   2891\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[0;32m-> 2892\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3695\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3693\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_for_each_replica\u001b[39m(\u001b[39mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m   3694\u001b[0m   \u001b[39mwith\u001b[39;00m ReplicaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m-> 3695\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:601\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    600\u001b[0m   \u001b[39mwith\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mControlStatusCtx(status\u001b[39m=\u001b[39mag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 601\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/keras/engine/training.py:860\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[0;32m--> 860\u001b[0m   outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m    861\u001b[0m   \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m    862\u001b[0m   \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/keras/engine/training.py:816\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    812\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    813\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTarget data is missing. Your model has `loss`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    814\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mand therefore expects target data to be passed in `fit()`.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    815\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n\u001b[0;32m--> 816\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mminimize(loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainable_variables, tape\u001b[39m=\u001b[39;49mtape)\n\u001b[1;32m    817\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiled_metrics\u001b[39m.\u001b[39mupdate_state(y, y_pred, sample_weight)\n\u001b[1;32m    818\u001b[0m \u001b[39m# Collect metrics to return\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:532\u001b[0m, in \u001b[0;36mOptimizerV2.minimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[39m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \n\u001b[1;32m    502\u001b[0m \u001b[39mThis method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \n\u001b[1;32m    529\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_gradients(\n\u001b[1;32m    531\u001b[0m     loss, var_list\u001b[39m=\u001b[39mvar_list, grad_loss\u001b[39m=\u001b[39mgrad_loss, tape\u001b[39m=\u001b[39mtape)\n\u001b[0;32m--> 532\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_gradients(grads_and_vars, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:672\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    669\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform_gradients(grads_and_vars)\n\u001b[1;32m    671\u001b[0m \u001b[39mif\u001b[39;00m optimizer_utils\u001b[39m.\u001b[39mstrategy_supports_no_merge_call():\n\u001b[0;32m--> 672\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distributed_apply(strategy, grads_and_vars, name,\n\u001b[1;32m    673\u001b[0m                                  apply_state)\n\u001b[1;32m    674\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_replica_context()\u001b[39m.\u001b[39mmerge_call(\n\u001b[1;32m    676\u001b[0m       functools\u001b[39m.\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distributed_apply, apply_state\u001b[39m=\u001b[39mapply_state),\n\u001b[1;32m    677\u001b[0m       args\u001b[39m=\u001b[39m(grads_and_vars,),\n\u001b[1;32m    678\u001b[0m       kwargs\u001b[39m=\u001b[39m{\n\u001b[1;32m    679\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: name,\n\u001b[1;32m    680\u001b[0m       })\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:721\u001b[0m, in \u001b[0;36mOptimizerV2._distributed_apply\u001b[0;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[39mwith\u001b[39;00m distribution\u001b[39m.\u001b[39mextended\u001b[39m.\u001b[39mcolocate_vars_with(var):\n\u001b[1;32m    718\u001b[0m   \u001b[39mwith\u001b[39;00m name_scope_only_in_function_or_graph(\n\u001b[1;32m    719\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mupdate\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m eagerly_outside_functions \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mupdate_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[1;32m    720\u001b[0m       var\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mname):\n\u001b[0;32m--> 721\u001b[0m     update_op \u001b[39m=\u001b[39m distribution\u001b[39m.\u001b[39;49mextended\u001b[39m.\u001b[39;49mupdate(\n\u001b[1;32m    722\u001b[0m         var, apply_grad_to_update_var, args\u001b[39m=\u001b[39;49m(grad,), group\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    723\u001b[0m     \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39min_cross_replica_context():\n\u001b[1;32m    724\u001b[0m       \u001b[39m# In cross-replica context, extended.update returns a list of\u001b[39;00m\n\u001b[1;32m    725\u001b[0m       \u001b[39m# update ops from all replicas (group=False).\u001b[39;00m\n\u001b[1;32m    726\u001b[0m       update_ops\u001b[39m.\u001b[39mextend(update_op)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2636\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2634\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update(var, fn, args, kwargs, group)\n\u001b[1;32m   2635\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2636\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_replica_ctx_update(\n\u001b[1;32m   2637\u001b[0m       var, fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs, group\u001b[39m=\u001b[39;49mgroup)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2515\u001b[0m, in \u001b[0;36mStrategyExtendedV2._replica_ctx_update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2512\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge_fn\u001b[39m(_, \u001b[39m*\u001b[39mmerged_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmerged_kwargs):\n\u001b[1;32m   2513\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(var, fn, merged_args, merged_kwargs, group\u001b[39m=\u001b[39mgroup)\n\u001b[0;32m-> 2515\u001b[0m \u001b[39mreturn\u001b[39;00m replica_context\u001b[39m.\u001b[39;49mmerge_call(merge_fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3107\u001b[0m, in \u001b[0;36mReplicaContextBase.merge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3103\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m   3105\u001b[0m merge_fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m   3106\u001b[0m     merge_fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 3107\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_merge_call(merge_fn, args, kwargs)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3114\u001b[0m, in \u001b[0;36mReplicaContextBase._merge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3111\u001b[0m _push_per_thread_mode(  \u001b[39m# thread-local, so not needed with multiple threads\u001b[39;00m\n\u001b[1;32m   3112\u001b[0m     distribution_strategy_context\u001b[39m.\u001b[39m_CrossReplicaThreadMode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy))  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   3113\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3114\u001b[0m   \u001b[39mreturn\u001b[39;00m merge_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   3115\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   3116\u001b[0m   _pop_per_thread_mode()\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:601\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    600\u001b[0m   \u001b[39mwith\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mControlStatusCtx(status\u001b[39m=\u001b[39mag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 601\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2513\u001b[0m, in \u001b[0;36mStrategyExtendedV2._replica_ctx_update.<locals>.merge_fn\u001b[0;34m(_, *merged_args, **merged_kwargs)\u001b[0m\n\u001b[1;32m   2512\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge_fn\u001b[39m(_, \u001b[39m*\u001b[39mmerged_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmerged_kwargs):\n\u001b[0;32m-> 2513\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(var, fn, merged_args, merged_kwargs, group\u001b[39m=\u001b[39;49mgroup)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2634\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2631\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m   2632\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   2633\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[0;32m-> 2634\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update(var, fn, args, kwargs, group)\n\u001b[1;32m   2635\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2636\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   2637\u001b[0m       var, fn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs, group\u001b[39m=\u001b[39mgroup)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3709\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3706\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update\u001b[39m(\u001b[39mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   3707\u001b[0m   \u001b[39m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   3708\u001b[0m   \u001b[39m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 3709\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_non_slot(var, fn, (var,) \u001b[39m+\u001b[39;49m \u001b[39mtuple\u001b[39;49m(args), kwargs, group)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3715\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   3711\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_non_slot\u001b[39m(\u001b[39mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   3712\u001b[0m   \u001b[39m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   3713\u001b[0m   \u001b[39m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   3714\u001b[0m   \u001b[39mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 3715\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   3716\u001b[0m     \u001b[39mif\u001b[39;00m should_group:\n\u001b[1;32m   3717\u001b[0m       \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:601\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    600\u001b[0m   \u001b[39mwith\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mControlStatusCtx(status\u001b[39m=\u001b[39mag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 601\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:699\u001b[0m, in \u001b[0;36mOptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m    697\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mapply_state\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse_apply_args:\n\u001b[1;32m    698\u001b[0m     apply_kwargs[\u001b[39m\"\u001b[39m\u001b[39mapply_state\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m apply_state\n\u001b[0;32m--> 699\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resource_apply_sparse_duplicate_indices(\n\u001b[1;32m    700\u001b[0m       grad\u001b[39m.\u001b[39;49mvalues, var, grad\u001b[39m.\u001b[39;49mindices, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mapply_kwargs)\n\u001b[1;32m    702\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mapply_state\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dense_apply_args:\n\u001b[1;32m    703\u001b[0m   apply_kwargs[\u001b[39m\"\u001b[39m\u001b[39mapply_state\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m apply_state\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:1283\u001b[0m, in \u001b[0;36mOptimizerV2._resource_apply_sparse_duplicate_indices\u001b[0;34m(self, grad, handle, indices, **kwargs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_resource_apply_sparse_duplicate_indices\u001b[39m(\u001b[39mself\u001b[39m, grad, handle, indices,\n\u001b[1;32m   1261\u001b[0m                                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1262\u001b[0m   \u001b[39m\"\"\"Add ops to apply sparse gradients to `handle`, with repeated indices.\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m \n\u001b[1;32m   1264\u001b[0m \u001b[39m  Optimizers which override this method must deal with repeated indices. See\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[39m    An `Operation` which updates the value of the variable.\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1283\u001b[0m   summed_grad, unique_indices \u001b[39m=\u001b[39m _deduplicate_indexed_slices(\n\u001b[1;32m   1284\u001b[0m       values\u001b[39m=\u001b[39;49mgrad, indices\u001b[39m=\u001b[39;49mindices)\n\u001b[1;32m   1285\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resource_apply_sparse(summed_grad, handle, unique_indices,\n\u001b[1;32m   1286\u001b[0m                                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:58\u001b[0m, in \u001b[0;36m_deduplicate_indexed_slices\u001b[0;34m(values, indices)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_deduplicate_indexed_slices\u001b[39m(values, indices):\n\u001b[1;32m     46\u001b[0m   \u001b[39m\"\"\"Sums `values` associated with any non-unique `indices`.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39m    `values` slices associated with each unique index.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m   unique_indices, new_index_positions \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49munique(indices)\n\u001b[1;32m     59\u001b[0m   summed_values \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39munsorted_segment_sum(\n\u001b[1;32m     60\u001b[0m       values, new_index_positions,\n\u001b[1;32m     61\u001b[0m       tf\u001b[39m.\u001b[39mshape(unique_indices)[\u001b[39m0\u001b[39m])\n\u001b[1;32m     62\u001b[0m   \u001b[39mreturn\u001b[39;00m (summed_values, unique_indices)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1096\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1097\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1098\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:2012\u001b[0m, in \u001b[0;36munique\u001b[0;34m(x, out_idx, name)\u001b[0m\n\u001b[1;32m   1973\u001b[0m \u001b[39m\"\"\"Finds unique elements in a 1-D tensor.\u001b[39;00m\n\u001b[1;32m   1974\u001b[0m \n\u001b[1;32m   1975\u001b[0m \u001b[39mSee also `tf.unique_with_counts`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2006\u001b[0m \n\u001b[1;32m   2007\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2008\u001b[0m \u001b[39m# TODO(yongtang): switch to v2 once API deprecation\u001b[39;00m\n\u001b[1;32m   2009\u001b[0m \u001b[39m# period (3 weeks) pass.\u001b[39;00m\n\u001b[1;32m   2010\u001b[0m \u001b[39m# TODO(yongtang): The documentation should also\u001b[39;00m\n\u001b[1;32m   2011\u001b[0m \u001b[39m# be updated when switch  to v2.\u001b[39;00m\n\u001b[0;32m-> 2012\u001b[0m \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49munique(x, out_idx, name)\n",
      "File \u001b[0;32m~/.miniconda3/envs/cs1470/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py:11947\u001b[0m, in \u001b[0;36munique\u001b[0;34m(x, out_idx, name)\u001b[0m\n\u001b[1;32m  11945\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m  11946\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m> 11947\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m  11948\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mUnique\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, x, \u001b[39m\"\u001b[39;49m\u001b[39mout_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m, out_idx)\n\u001b[1;32m  11949\u001b[0m     _result \u001b[39m=\u001b[39m _UniqueOutput\u001b[39m.\u001b[39m_make(_result)\n\u001b[1;32m  11950\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_filepath = 'checkpoints/vocab/run_2/'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "history = model.fit(\n",
    "                    (train_df['full_text'], np.array(train_df['log_freqs'].values.tolist())),\n",
    "                    train_df['vocabulary'],\n",
    "                    validation_data = ((valid_df['full_text'], np.array(valid_df['log_freqs'].values.tolist())), valid_df['vocabulary']),\n",
    "                    steps_per_epoch= train_df.shape[0]//8,\n",
    "                    batch_size = 8,\n",
    "                    epochs= 25,\n",
    "                    verbose = 1,\n",
    "                    shuffle= True, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.605384349822998,\n",
       "  0.43926531076431274,\n",
       "  0.43117430806159973,\n",
       "  0.4231126010417938,\n",
       "  0.4112090766429901,\n",
       "  0.39481255412101746,\n",
       "  0.37830406427383423,\n",
       "  0.3676895201206207,\n",
       "  0.36019647121429443,\n",
       "  0.3509618937969208,\n",
       "  0.3448069095611572,\n",
       "  0.3376022279262543,\n",
       "  0.33415818214416504],\n",
       " 'MCRMSE': [0.605384349822998,\n",
       "  0.43926531076431274,\n",
       "  0.43117430806159973,\n",
       "  0.4231126010417938,\n",
       "  0.4112090766429901,\n",
       "  0.39481255412101746,\n",
       "  0.37830406427383423,\n",
       "  0.3676895201206207,\n",
       "  0.36019647121429443,\n",
       "  0.3509618937969208,\n",
       "  0.3448069095611572,\n",
       "  0.3376022279262543,\n",
       "  0.33415818214416504],\n",
       " 'val_loss': [0.42399463057518005,\n",
       "  0.41263145208358765,\n",
       "  0.4066115915775299,\n",
       "  0.39771413803100586,\n",
       "  0.38454118371009827,\n",
       "  0.3781641125679016,\n",
       "  0.36460965871810913,\n",
       "  0.3625567555427551,\n",
       "  0.3951273262500763,\n",
       "  0.3822939991950989,\n",
       "  0.3597392141819,\n",
       "  0.38560110330581665,\n",
       "  0.3674958348274231],\n",
       " 'val_MCRMSE': [0.4237264394760132,\n",
       "  0.4124107360839844,\n",
       "  0.4062783718109131,\n",
       "  0.39738544821739197,\n",
       "  0.38420167565345764,\n",
       "  0.37778380513191223,\n",
       "  0.3642725348472595,\n",
       "  0.3621968924999237,\n",
       "  0.3946959376335144,\n",
       "  0.38193145394325256,\n",
       "  0.35940834879875183,\n",
       "  0.3852645754814148,\n",
       "  0.36706170439720154]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).to_csv(os.path.join(checkpoint_filepath, 'loss_history.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs1470",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d7a279b3fb804aa2a488e25e21d217aa4e6f910ce24c7abf709fdf3600e70da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
