{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "vscode": {
   "interpreter": {
    "hash": "5d7a279b3fb804aa2a488e25e21d217aa4e6f910ce24c7abf709fdf3600e70da"
   }
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.data import load\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "#from transformer import positional_encoding, EncoderLayer\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "from transformers import BertTokenizer , TFBertModel\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T15:28:03.583278Z",
     "iopub.execute_input": "2022-12-09T15:28:03.584257Z",
     "iopub.status.idle": "2022-12-09T15:28:18.378876Z",
     "shell.execute_reply.started": "2022-12-09T15:28:03.584149Z",
     "shell.execute_reply": "2022-12-09T15:28:18.377847Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/input/feedback-prize-english-language-learning/sample_submission.csv\n/kaggle/input/feedback-prize-english-language-learning/train.csv\n/kaggle/input/feedback-prize-english-language-learning/test.csv\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/config.json\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/tokenizer.json\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/tf_model.h5\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/vocab.txt\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/flax_model.msgpack\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/whole-word-masking/._bert_config.json\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/whole-word-masking/bert_config.json\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/whole-word-masking/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/384-8bits.tflite\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/tokenizer.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/tf_model.h5\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/384-fp16.tflite\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/vocab.txt\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/384.tflite\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/saved_model.pb\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/._variables\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/._assets\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/._saved_model.pb\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/._.\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/variables/variables.index\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/variables/._variables.index\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/variables/._variables.data-00000-of-00001\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/variables/variables.data-00000-of-00001\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/PaxHeader/assets\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/PaxHeader/variables\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/PaxHeader/currentdir\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/config.json\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/tokenizer.json\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/tf_model.h5\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/vocab.txt\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/flax_model.msgpack\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/whole-word-masking/._bert_config.json\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/whole-word-masking/bert_config.json\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/whole-word-masking/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/bert-base-cased/bert-base-cased/config.json\n/kaggle/input/huggingface-bert-variants/bert-base-cased/bert-base-cased/tokenizer.json\n/kaggle/input/huggingface-bert-variants/bert-base-cased/bert-base-cased/tf_model.h5\n/kaggle/input/huggingface-bert-variants/bert-base-cased/bert-base-cased/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/bert-base-cased/bert-base-cased/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/bert-base-cased/bert-base-cased/vocab.txt\n/kaggle/input/huggingface-bert-variants/bert-base-cased/bert-base-cased/flax_model.msgpack\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased/rust_model.ot\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased/config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased/tokenizer.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased/tf_model.h5\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased/vocab.txt\n/kaggle/input/huggingface-bert-variants/distilbert-base-multilingual-cased/distilbert-base-multilingual-cased/config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-multilingual-cased/distilbert-base-multilingual-cased/tokenizer.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-multilingual-cased/distilbert-base-multilingual-cased/tf_model.h5\n/kaggle/input/huggingface-bert-variants/distilbert-base-multilingual-cased/distilbert-base-multilingual-cased/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-multilingual-cased/distilbert-base-multilingual-cased/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-multilingual-cased/distilbert-base-multilingual-cased/vocab.txt\n/kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased/rust_model.ot\n/kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased/config.json\n/kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased/tokenizer.json\n/kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased/tf_model.h5\n/kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased/vocab.txt\n/kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased/flax_model.msgpack\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/rust_model.ot\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tokenizer.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tf_model.h5\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/vocab.txt\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/saved_model/saved_model.pb\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/saved_model/variables/variables.index\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/saved_model/variables/variables.data-00000-of-00001\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard10of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard40of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard2of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard4of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard57of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard48of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard23of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard22of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard45of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard19of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard39of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard38of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard56of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard41of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard55of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard32of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard15of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard16of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard17of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard36of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard18of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard27of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard58of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard6of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard61of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard53of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard14of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard46of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard33of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard62of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard47of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard54of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard63of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard12of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard8of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard43of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard59of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard13of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard30of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard25of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard1of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard51of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard42of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard7of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard9of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard49of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/model.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard21of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard34of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard3of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard24of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard28of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard31of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard35of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard5of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard20of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard26of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard50of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard29of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard44of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard11of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard37of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard52of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard60of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased/distilbert-base-cased/config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased/distilbert-base-cased/tokenizer.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased/distilbert-base-cased/tf_model.h5\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased/distilbert-base-cased/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased/distilbert-base-cased/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased/distilbert-base-cased/vocab.txt\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2022-12-09 15:28:13.334787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 15:28:13.336028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 15:28:13.336764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 15:28:13.338652: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-09 15:28:13.338970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 15:28:13.339664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 15:28:13.340334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 15:28:17.998530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 15:28:17.999403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 15:28:18.000089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 15:28:18.000712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15043 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# adapted from https://www.tensorflow.org/text/tutorials/transformer\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]  # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth  # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000 ** depths)  # (1, depth)\n",
    "    angle_rads = positions * angle_rates  # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads,\n",
    "                 dff, vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(\n",
    "            vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # `x` is token-IDs shape: (batch, seq_len)\n",
    "        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`.\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T15:28:28.107360Z",
     "iopub.execute_input": "2022-12-09T15:28:28.107998Z",
     "iopub.status.idle": "2022-12-09T15:28:28.131883Z",
     "shell.execute_reply.started": "2022-12-09T15:28:28.107961Z",
     "shell.execute_reply": "2022-12-09T15:28:28.130678Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data_dir = \"/kaggle/input/\"\n",
    "bert_dir = \"/kaggle/input/huggingface-bert-variants/bert-base-uncased/\"\n",
    "train_df = pd.read_csv(data_dir + 'feedback-prize-english-language-learning/train.csv')\n",
    "test_df = pd.read_csv(data_dir + 'feedback-prize-english-language-learning/test.csv')\n",
    "sample_df = pd.read_csv(data_dir + 'feedback-prize-english-language-learning/sample_submission.csv')\n",
    "bert_path = bert_dir + 'bert-base-uncased'\n",
    "print(train_df.shape, test_df.shape, sample_df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T15:28:28.707609Z",
     "iopub.execute_input": "2022-12-09T15:28:28.708269Z",
     "iopub.status.idle": "2022-12-09T15:28:28.967157Z",
     "shell.execute_reply.started": "2022-12-09T15:28:28.708234Z",
     "shell.execute_reply": "2022-12-09T15:28:28.966104Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "(3911, 8) (3, 2) (3, 7)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "size = train_df.shape[0]\n",
    "train, validate = int(0.8*size), int(0.2*size)\n",
    "valid_df = train_df.tail(validate).copy()\n",
    "train_df = train_df.head(train).copy()\n",
    "print(train_df.shape, valid_df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T15:28:30.392456Z",
     "iopub.execute_input": "2022-12-09T15:28:30.393149Z",
     "iopub.status.idle": "2022-12-09T15:28:30.400446Z",
     "shell.execute_reply.started": "2022-12-09T15:28:30.393112Z",
     "shell.execute_reply": "2022-12-09T15:28:30.399289Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "(3128, 8) (782, 8)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Merging Train and Test Data\n",
    "train_size = train_df.shape[0]\n",
    "test_size = test_df.shape[0]\n",
    "print(train_df.shape, test_df.shape, valid_df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T15:28:33.238850Z",
     "iopub.execute_input": "2022-12-09T15:28:33.239210Z",
     "iopub.status.idle": "2022-12-09T15:28:33.245462Z",
     "shell.execute_reply.started": "2022-12-09T15:28:33.239180Z",
     "shell.execute_reply": "2022-12-09T15:28:33.244178Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "(3128, 8) (3, 2) (782, 8)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text) :\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+','', text)\n",
    "    text = re.sub(r'@[0-9a-zA-Z]*\\W+',' ' , text)\n",
    "\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub(r'\\#', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "\n",
    "    list_text = text.split()\n",
    "    text = ' '.join(list_text)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T15:28:33.771796Z",
     "iopub.execute_input": "2022-12-09T15:28:33.772157Z",
     "iopub.status.idle": "2022-12-09T15:28:33.779920Z",
     "shell.execute_reply.started": "2022-12-09T15:28:33.772124Z",
     "shell.execute_reply": "2022-12-09T15:28:33.778901Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "def encode(input_text):\n",
    "    inputs = tokenizer.batch_encode_plus(input_text,padding='max_length',max_length=max_text_len, truncation=True)\n",
    "    return inputs"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T15:46:51.420794Z",
     "iopub.execute_input": "2022-12-09T15:46:51.421155Z",
     "iopub.status.idle": "2022-12-09T15:46:51.468967Z",
     "shell.execute_reply.started": "2022-12-09T15:46:51.421124Z",
     "shell.execute_reply": "2022-12-09T15:46:51.467969Z"
    },
    "trusted": true
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text_vocab = set()\n",
    "pos_vocab = list(load('help/tagsets/upenn_tagset.pickle').keys())\n",
    "max_text_len = 0\n",
    "truncate_to = 512\n",
    "for dataset in [train_df, valid_df, test_df]:\n",
    "    #dataset.drop(['text_id'], axis=1, inplace=True)\n",
    "    dataset['full_text'] = dataset['full_text'].apply(lambda text : preprocess(text))\n",
    "    dataset['pos_tag'] = dataset['full_text'].apply(lambda text: pos_tag(word_tokenize(text)))\n",
    "    # there are 36 possible pos_tags\n",
    "    dataset['pos'] = dataset['pos_tag'].apply(lambda text: ' '.join([elem[1] for elem in text[:truncate_to]]))\n",
    "    dataset['tokens'] = dataset['pos_tag'].apply(lambda text: [elem[0] for elem in text[:truncate_to]])\n",
    "    for tokens in dataset['tokens']:\n",
    "        text_vocab.update(tokens)\n",
    "        max_text_len = max(max_text_len, len(tokens))\n",
    "    dataset['tokens'] = dataset['tokens'].apply(lambda text: ' '.join(text))\n",
    "#     dataset.drop(['full_text'], axis=1, inplace=True)\n",
    "    dataset.drop(['pos_tag'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T15:29:28.567505Z",
     "iopub.execute_input": "2022-12-09T15:29:28.567893Z",
     "iopub.status.idle": "2022-12-09T15:30:46.880034Z",
     "shell.execute_reply.started": "2022-12-09T15:29:28.567861Z",
     "shell.execute_reply": "2022-12-09T15:30:46.878970Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_bert = encode(train_df['tokens'].values.tolist())['input_ids']\n",
    "valid_bert = encode(valid_df['tokens'].values.tolist())['input_ids']\n",
    "test_bert = encode(test_df['tokens'].values.tolist())['input_ids']"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T15:46:55.483732Z",
     "iopub.execute_input": "2022-12-09T15:46:55.484092Z",
     "iopub.status.idle": "2022-12-09T15:47:34.636662Z",
     "shell.execute_reply.started": "2022-12-09T15:46:55.484061Z",
     "shell.execute_reply": "2022-12-09T15:47:34.635692Z"
    },
    "trusted": true
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df['bert'] = train_bert\n",
    "valid_df['bert'] = valid_bert\n",
    "test_df['bert'] = test_bert"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T15:47:46.123556Z",
     "iopub.execute_input": "2022-12-09T15:47:46.124452Z",
     "iopub.status.idle": "2022-12-09T15:47:46.132686Z",
     "shell.execute_reply.started": "2022-12-09T15:47:46.124406Z",
     "shell.execute_reply": "2022-12-09T15:47:46.131633Z"
    },
    "trusted": true
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(train_df.iloc[0]['tokens'].split())\n",
    "len(train_bert[0])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T15:47:46.893455Z",
     "iopub.execute_input": "2022-12-09T15:47:46.893929Z",
     "iopub.status.idle": "2022-12-09T15:47:46.902243Z",
     "shell.execute_reply.started": "2022-12-09T15:47:46.893892Z",
     "shell.execute_reply": "2022-12-09T15:47:46.901278Z"
    },
    "trusted": true
   },
   "execution_count": 29,
   "outputs": [
    {
     "execution_count": 29,
     "output_type": "execute_result",
     "data": {
      "text/plain": "512"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "all_data = pd.concat((train_df, valid_df, test_df)).reset_index(drop=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T15:47:47.735077Z",
     "iopub.execute_input": "2022-12-09T15:47:47.735495Z",
     "iopub.status.idle": "2022-12-09T15:47:47.760336Z",
     "shell.execute_reply.started": "2022-12-09T15:47:47.735459Z",
     "shell.execute_reply": "2022-12-09T15:47:47.759477Z"
    },
    "trusted": true
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "max_text_len"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T15:47:48.268048Z",
     "iopub.execute_input": "2022-12-09T15:47:48.268750Z",
     "iopub.status.idle": "2022-12-09T15:47:48.275494Z",
     "shell.execute_reply.started": "2022-12-09T15:47:48.268710Z",
     "shell.execute_reply": "2022-12-09T15:47:48.274427Z"
    },
    "trusted": true
   },
   "execution_count": 31,
   "outputs": [
    {
     "execution_count": 31,
     "output_type": "execute_result",
     "data": {
      "text/plain": "512"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# tag parts of speech, add as feature\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T15:47:48.822498Z",
     "iopub.execute_input": "2022-12-09T15:47:48.823508Z",
     "iopub.status.idle": "2022-12-09T15:47:48.854855Z",
     "shell.execute_reply.started": "2022-12-09T15:47:48.823468Z",
     "shell.execute_reply": "2022-12-09T15:47:48.853740Z"
    },
    "trusted": true
   },
   "execution_count": 32,
   "outputs": [
    {
     "execution_count": 32,
     "output_type": "execute_result",
     "data": {
      "text/plain": "        text_id                                          full_text  cohesion  \\\n0  0016926B079C  i think that students would benefit from learn...       3.5   \n1  0022683E9EA5  when a problem is a change you have to let it ...       2.5   \n2  00299B378633  dear, principal if u change the school policy ...       3.0   \n3  003885A45F42  the best time in life is when you become yours...       4.5   \n4  0049B1DF5CCC  small act of kindness can impact in other peop...       2.5   \n\n   syntax  vocabulary  phraseology  grammar  conventions  \\\n0     3.5         3.0          3.0      4.0          3.0   \n1     2.5         3.0          2.0      2.0          2.5   \n2     3.5         3.0          3.0      3.0          2.5   \n3     4.5         4.5          4.5      4.0          5.0   \n4     3.0         3.0          3.0      2.5          2.5   \n\n                                                 pos  \\\n0  NN VBP IN NNS MD VB IN VBG IN NN , IN PRP VBP ...   \n1  WRB DT NN VBZ DT NN PRP VBP TO VB PRP VB DT JJ...   \n2  NN , JJ IN JJ VBP DT NN NN IN VBG DT NN NN NN ...   \n3  DT JJS NN IN NN VBZ WRB PRP VBP PRP . VB VBP I...   \n4  JJ NN IN NN MD VB IN JJ NNS MD VB NNS TO VB JJ...   \n\n                                              tokens  \\\n0  i think that students would benefit from learn...   \n1  when a problem is a change you have to let it ...   \n2  dear , principal if u change the school policy...   \n3  the best time in life is when you become yours...   \n4  small act of kindness can impact in other peop...   \n\n                                                bert  \n0  [101, 1045, 2228, 2008, 2493, 2052, 5770, 2013...  \n1  [101, 2043, 1037, 3291, 2003, 1037, 2689, 2017...  \n2  [101, 6203, 1010, 4054, 2065, 1057, 2689, 1996...  \n3  [101, 1996, 2190, 2051, 1999, 2166, 2003, 2043...  \n4  [101, 2235, 2552, 1997, 16056, 2064, 4254, 199...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>full_text</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n      <th>pos</th>\n      <th>tokens</th>\n      <th>bert</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0016926B079C</td>\n      <td>i think that students would benefit from learn...</td>\n      <td>3.5</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>NN VBP IN NNS MD VB IN VBG IN NN , IN PRP VBP ...</td>\n      <td>i think that students would benefit from learn...</td>\n      <td>[101, 1045, 2228, 2008, 2493, 2052, 5770, 2013...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0022683E9EA5</td>\n      <td>when a problem is a change you have to let it ...</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.5</td>\n      <td>WRB DT NN VBZ DT NN PRP VBP TO VB PRP VB DT JJ...</td>\n      <td>when a problem is a change you have to let it ...</td>\n      <td>[101, 2043, 1037, 3291, 2003, 1037, 2689, 2017...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00299B378633</td>\n      <td>dear, principal if u change the school policy ...</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>NN , JJ IN JJ VBP DT NN NN IN VBG DT NN NN NN ...</td>\n      <td>dear , principal if u change the school policy...</td>\n      <td>[101, 6203, 1010, 4054, 2065, 1057, 2689, 1996...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>003885A45F42</td>\n      <td>the best time in life is when you become yours...</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>DT JJS NN IN NN VBZ WRB PRP VBP PRP . VB VBP I...</td>\n      <td>the best time in life is when you become yours...</td>\n      <td>[101, 1996, 2190, 2051, 1999, 2166, 2003, 2043...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0049B1DF5CCC</td>\n      <td>small act of kindness can impact in other peop...</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>JJ NN IN NN MD VB IN JJ NNS MD VB NNS TO VB JJ...</td>\n      <td>small act of kindness can impact in other peop...</td>\n      <td>[101, 2235, 2552, 1997, 16056, 2064, 4254, 199...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# \"(E)lement-(Wi)se (Dense)\" Layer for combining two embeddings (or other multi-feature time sequence data) with a Dense layer applied element-wise (so not exactly Dense, as in the output embedding the first position is only determined by a linear combination of the two embedding values in corresponding positions in the two input embeddings)\n",
    "# (We picked this name because it was funny)\n",
    "class EWiDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, activation=None, **kwargs):\n",
    "        super(EWiDense, self).__init__(**kwargs)\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embedding_size = input_shape[0][-1]\n",
    "        # print(self.tile_shape)\n",
    "        self.w1 = self.add_weight(\n",
    "            shape=[self.embedding_size],\n",
    "            initializer=\"ones\",\n",
    "            trainable=True,\n",
    "            name=\"w1\"\n",
    "        )\n",
    "        # print(tf.shape(self.w1))\n",
    "        self.w2 = self.add_weight(\n",
    "            shape=[self.embedding_size],\n",
    "            initializer=\"ones\",\n",
    "            trainable=True,\n",
    "            name=\"w2\"\n",
    "        )\n",
    "        self.b1 = self.add_weight(\n",
    "            shape=[self.embedding_size],\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "            name=\"b1\"\n",
    "        )\n",
    "\n",
    "    def call(self, data):  # expected x of two embeddings of shape batch_size, seq_len, embedding_size\n",
    "        if self.activation:\n",
    "            return self.activation(tf.multiply(data[0], self.w1) + tf.multiply(data[1], self.w2) + self.b1)\n",
    "        return tf.multiply(data[0], self.w1) + tf.multiply(data[1], self.w2) + self.b1"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T15:47:56.256713Z",
     "iopub.execute_input": "2022-12-09T15:47:56.257093Z",
     "iopub.status.idle": "2022-12-09T15:47:56.267329Z",
     "shell.execute_reply.started": "2022-12-09T15:47:56.257061Z",
     "shell.execute_reply": "2022-12-09T15:47:56.266125Z"
    },
    "trusted": true
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# compare standard positional encoding with grammar + positional encodinng\n",
    "# use encoder networks, but not the decoders because we don't have an output sequence really\n",
    "\n",
    "class GrammarModel(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               max_text_len, text_vocab, pos_vocab, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.text_vectorization = tf.keras.layers.TextVectorization(output_mode='int', output_sequence_length=max_text_len)\n",
    "            self.text_vectorization.adapt(text_vocab)\n",
    "            self.pos_vectorization = tf.keras.layers.TextVectorization(output_mode='int', output_sequence_length=max_text_len)\n",
    "            self.pos_vectorization.adapt(pos_vocab)\n",
    "            self.word_embedding = tf.keras.layers.Embedding(self.text_vectorization.vocabulary_size(), d_model) # replace\n",
    "            self.pos_embedding = tf.keras.layers.Embedding(self.pos_vectorization.vocabulary_size(), d_model)\n",
    "        self.EWiDenseLayer = EWiDense(activation=tf.keras.layers.LeakyReLU())\n",
    "        self.pos_encoding = tf.Variable(positional_encoding(length=max_text_len, depth=d_model), trainable=False)\n",
    "        self.pos_scalar = tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.dense = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        words = inputs[:, 0]\n",
    "        pos = inputs[:, 1]\n",
    "        # combine embeddings\n",
    "        words = self.text_vectorization(words)\n",
    "        pos = self.pos_vectorization(pos)\n",
    "        x = self.EWiDenseLayer((self.word_embedding(words), self.pos_embedding(pos)))\n",
    "        # add positional encoding\n",
    "        x = x * self.pos_scalar\n",
    "        x = x + self.pos_encoding\n",
    "        # dropout\n",
    "        x = self.dropout(x)\n",
    "        # add encoding layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.layernorm(x)\n",
    "        return self.dense(x)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T15:47:57.232022Z",
     "iopub.execute_input": "2022-12-09T15:47:57.232407Z",
     "iopub.status.idle": "2022-12-09T15:47:57.246912Z",
     "shell.execute_reply.started": "2022-12-09T15:47:57.232375Z",
     "shell.execute_reply": "2022-12-09T15:47:57.245719Z"
    },
    "trusted": true
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class SyntaxModel(GrammarModel):\n",
    "    def __init__(self, hidden_size, *args, **kwargs):\n",
    "        super(SyntaxModel, self).__init__(*args, **kwargs)\n",
    "        self.inter_dense = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "        self.syntax_dense = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        words = inputs[:, 0]\n",
    "        pos = inputs[:, 1]\n",
    "        # combine embeddings\n",
    "        words = self.text_vectorization(words)\n",
    "        pos = self.pos_vectorization(pos)\n",
    "        x = self.EWiDenseLayer((self.word_embedding(words), self.pos_embedding(pos)))\n",
    "        # add positional encoding\n",
    "        x = x * self.pos_scalar\n",
    "        x = x + self.pos_encoding\n",
    "        # dropout\n",
    "        x = self.dropout(x)\n",
    "        # add encoding layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.layernorm(x)\n",
    "        self.encoder_out = x #(batch_size, embed_size)\n",
    "        grammar_out = self.dense(x) #(batch_size, 1)\n",
    "        \n",
    "        concatenated = tf.concat((self.encoder_out, grammar_out), axis=1)\n",
    "        x = self.inter_dense(concatenated)\n",
    "        syntax_out = self.syntax_dense(x)\n",
    "        return tf.concat((grammar_out, syntax_out), axis=1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.252506Z",
     "iopub.execute_input": "2022-12-09T05:44:06.253109Z",
     "iopub.status.idle": "2022-12-09T05:44:06.263439Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.253077Z",
     "shell.execute_reply": "2022-12-09T05:44:06.262665Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class BERTSyntaxModel(GrammarModel):\n",
    "    def __init__(self, hidden_size, *args, **kwargs):\n",
    "        super(BERTSyntaxModel, self).__init__(*args, **kwargs)\n",
    "        del self.word_embedding\n",
    "        del self.text_vectorization\n",
    "        self.inter_dense = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "        self.syntax_dense = tf.keras.layers.Dense(1)\n",
    "        self.bert_encoder = TFBertModel.from_pretrained(bert_path)\n",
    "        for param in self.bert_encoder.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.bert_down_size = tf.keras.layers.Dense(self.d_model)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        words = inputs[0]\n",
    "        pos = inputs[1]\n",
    "        # combine embeddings\n",
    "        embedding = self.bert_encoder(words, training=training)[0]\n",
    "        embedding = self.bert_down_size(embedding)\n",
    "        pos = self.pos_vectorization(pos)\n",
    "        x = self.EWiDenseLayer((embedding, self.pos_embedding(pos)))\n",
    "        # add positional encoding\n",
    "        x = x * self.pos_scalar\n",
    "        x = x + self.pos_encoding\n",
    "        # dropout\n",
    "        x = self.dropout(x)\n",
    "        # add encoding layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.layernorm(x)\n",
    "        self.encoder_out = x #(batch_size, embed_size)\n",
    "        grammar_out = self.dense(x) #(batch_size, 1)\n",
    "        \n",
    "        concatenated = tf.concat((self.encoder_out, grammar_out), axis=1)\n",
    "        x = self.inter_dense(concatenated)\n",
    "        syntax_out = self.syntax_dense(x)\n",
    "        return tf.concat((grammar_out, syntax_out), axis=1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T15:47:59.527425Z",
     "iopub.execute_input": "2022-12-09T15:47:59.528373Z",
     "iopub.status.idle": "2022-12-09T15:47:59.541231Z",
     "shell.execute_reply.started": "2022-12-09T15:47:59.528335Z",
     "shell.execute_reply": "2022-12-09T15:47:59.540278Z"
    },
    "trusted": true
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## Column-wise RMSE\n",
    "def MCRMSE(y_true, y_pred):\n",
    "    mcrmse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n",
    "    return tf.reduce_mean(tf.sqrt(mcrmse), axis=-1, keepdims=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T15:47:59.937246Z",
     "iopub.execute_input": "2022-12-09T15:47:59.937613Z",
     "iopub.status.idle": "2022-12-09T15:47:59.943179Z",
     "shell.execute_reply.started": "2022-12-09T15:47:59.937561Z",
     "shell.execute_reply": "2022-12-09T15:47:59.941846Z"
    },
    "trusted": true
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_model():\n",
    "    num_layers = 6\n",
    "    d_model = 64\n",
    "    dff = 256\n",
    "    num_heads = 8\n",
    "    dropout_rate = 0.2\n",
    "    model = GrammarModel(num_layers, d_model, num_heads, dff, max_text_len, np.array(list(text_vocab)), np.array(list(pos_vocab)), dropout_rate)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-5, clipnorm=1), loss=MCRMSE, metrics=MCRMSE, run_eagerly=True)\n",
    "    return model"
   ],
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T15:48:00.538328Z",
     "iopub.execute_input": "2022-12-09T15:48:00.538709Z",
     "iopub.status.idle": "2022-12-09T15:48:00.544975Z",
     "shell.execute_reply.started": "2022-12-09T15:48:00.538677Z",
     "shell.execute_reply": "2022-12-09T15:48:00.543956Z"
    },
    "trusted": true
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class TransformerScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, scale_lr, hidden_size, warmup_steps):\n",
    "        self.scale_lr = tf.cast(scale_lr, tf.float32)\n",
    "        self.hidden_size = tf.cast(hidden_size, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        lr = self.scale_lr / tf.sqrt(self.hidden_size)\n",
    "        lr *= tf.minimum(1 / tf.sqrt(step), step * self.warmup_steps ** (-1.5))\n",
    "        return lr \n",
    "\n",
    "def create_syntax_model():\n",
    "    num_layers = 6\n",
    "    d_model = 64\n",
    "    dff = 256\n",
    "    num_heads = 8\n",
    "    dropout_rate = 0.2\n",
    "    model = BERTSyntaxModel(128, num_layers, d_model, num_heads, dff, max_text_len, np.array(list(text_vocab)), np.array(list(pos_vocab)), dropout_rate)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, clipnorm=1), loss=MCRMSE, metrics=MCRMSE, run_eagerly=True)\n",
    "    return model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T15:48:17.949625Z",
     "iopub.execute_input": "2022-12-09T15:48:17.950094Z",
     "iopub.status.idle": "2022-12-09T15:48:17.969101Z",
     "shell.execute_reply.started": "2022-12-09T15:48:17.950053Z",
     "shell.execute_reply": "2022-12-09T15:48:17.968047Z"
    },
    "trusted": true
   },
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# model = create_model()\n",
    "# model.summary()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.311197Z",
     "iopub.execute_input": "2022-12-09T05:44:06.311783Z",
     "iopub.status.idle": "2022-12-09T05:44:06.325856Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.311749Z",
     "shell.execute_reply": "2022-12-09T05:44:06.324145Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# len(train_df.iloc[0][['tokens', 'pos']][0].split())"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.328124Z",
     "iopub.execute_input": "2022-12-09T05:44:06.329051Z",
     "iopub.status.idle": "2022-12-09T05:44:06.336637Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.329015Z",
     "shell.execute_reply": "2022-12-09T05:44:06.335164Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# model.text_vectorization.vocabulary_size()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.338130Z",
     "iopub.execute_input": "2022-12-09T05:44:06.339200Z",
     "iopub.status.idle": "2022-12-09T05:44:06.345490Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.339164Z",
     "shell.execute_reply": "2022-12-09T05:44:06.344072Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "max_text_len"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T15:48:10.347294Z",
     "iopub.execute_input": "2022-12-09T15:48:10.347681Z",
     "iopub.status.idle": "2022-12-09T15:48:10.353856Z",
     "shell.execute_reply.started": "2022-12-09T15:48:10.347648Z",
     "shell.execute_reply": "2022-12-09T15:48:10.352871Z"
    },
    "trusted": true
   },
   "execution_count": 38,
   "outputs": [
    {
     "execution_count": 38,
     "output_type": "execute_result",
     "data": {
      "text/plain": "512"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# model(tf.expand_dims(tf.convert_to_tensor(train_df.iloc[0][['tokens', 'pos']]), 0))\n",
    "# model.summary()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.364521Z",
     "iopub.execute_input": "2022-12-09T05:44:06.365876Z",
     "iopub.status.idle": "2022-12-09T05:44:06.370019Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.365817Z",
     "shell.execute_reply": "2022-12-09T05:44:06.368952Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# tf.debugging.disable_traceback_filtering()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.373353Z",
     "iopub.execute_input": "2022-12-09T05:44:06.374145Z",
     "iopub.status.idle": "2022-12-09T05:44:06.382182Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.374101Z",
     "shell.execute_reply": "2022-12-09T05:44:06.380267Z"
    },
    "trusted": true
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# import tensorflow as tf\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.383473Z",
     "iopub.execute_input": "2022-12-09T05:44:06.384132Z",
     "iopub.status.idle": "2022-12-09T05:44:06.388869Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.384090Z",
     "shell.execute_reply": "2022-12-09T05:44:06.387552Z"
    },
    "trusted": true
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# checkpoint_filepath = 'tmp/checkpoint'\n",
    "# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=checkpoint_filepath,\n",
    "#     monitor=\"val_loss\",\n",
    "#     verbose=1,\n",
    "#     save_weights_only=True,\n",
    "#     mode='min',\n",
    "#     save_best_only=True)\n",
    "# history = model.fit(\n",
    "#                     train_df[['tokens', 'pos']],\n",
    "#                     train_df['grammar'],\n",
    "#                     validation_data = (valid_df[['tokens', 'pos']], valid_df['grammar']),\n",
    "#                     steps_per_epoch= train_df.shape[0]//4,\n",
    "#                     batch_size = 4,\n",
    "#                     epochs= 100,\n",
    "#                     verbose = 1,\n",
    "#                     shuffle= True,\n",
    "#                     callbacks=[model_checkpoint_callback])"
   ],
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.398334Z",
     "iopub.execute_input": "2022-12-09T05:44:06.398935Z",
     "iopub.status.idle": "2022-12-09T05:44:06.404195Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.398900Z",
     "shell.execute_reply": "2022-12-09T05:44:06.402755Z"
    },
    "trusted": true
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# history"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.405798Z",
     "iopub.execute_input": "2022-12-09T05:44:06.406525Z",
     "iopub.status.idle": "2022-12-09T05:44:06.417283Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.406482Z",
     "shell.execute_reply": "2022-12-09T05:44:06.416200Z"
    },
    "trusted": true
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# model(tf.expand_dims(tf.convert_to_tensor(train_df.iloc[0][['tokens', 'pos']]), 0))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.418571Z",
     "iopub.execute_input": "2022-12-09T05:44:06.419235Z",
     "iopub.status.idle": "2022-12-09T05:44:06.425740Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.419199Z",
     "shell.execute_reply": "2022-12-09T05:44:06.424705Z"
    },
    "trusted": true
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "syntax_model = create_syntax_model()"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T15:48:21.030783Z",
     "iopub.execute_input": "2022-12-09T15:48:21.031236Z",
     "iopub.status.idle": "2022-12-09T15:48:30.251341Z",
     "shell.execute_reply.started": "2022-12-09T15:48:21.031194Z",
     "shell.execute_reply": "2022-12-09T15:48:30.250480Z"
    },
    "trusted": true
   },
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "text": "2022-12-09 15:48:21.178155: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\nSome layers from the model checkpoint at /kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at /kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# test = (np.array(train_df['bert'].values.tolist()), train_df['pos'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:14.382623Z",
     "iopub.execute_input": "2022-12-09T05:44:14.383406Z",
     "iopub.status.idle": "2022-12-09T05:44:14.388093Z",
     "shell.execute_reply.started": "2022-12-09T05:44:14.383368Z",
     "shell.execute_reply": "2022-12-09T05:44:14.386876Z"
    },
    "trusted": true
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# test[1].shape"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:14.390156Z",
     "iopub.execute_input": "2022-12-09T05:44:14.390616Z",
     "iopub.status.idle": "2022-12-09T05:44:14.398737Z",
     "shell.execute_reply.started": "2022-12-09T05:44:14.390578Z",
     "shell.execute_reply": "2022-12-09T05:44:14.397667Z"
    },
    "trusted": true
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# syntax_model(test)\n",
    "# syntax_model.summary()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:14.401379Z",
     "iopub.execute_input": "2022-12-09T05:44:14.401989Z",
     "iopub.status.idle": "2022-12-09T05:44:14.409640Z",
     "shell.execute_reply.started": "2022-12-09T05:44:14.401960Z",
     "shell.execute_reply": "2022-12-09T05:44:14.408677Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 2\n",
    "checkpoint_filepath = 'checkpoints_bert_final/'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "deez_nuts = syntax_model.fit(\n",
    "                    (np.array(train_df['bert'].values.tolist()), train_df['pos']),\n",
    "                    train_df[['grammar', 'syntax']],\n",
    "                    validation_data = ((np.array(valid_df['bert'].values.tolist()), valid_df['pos']), valid_df[['grammar', 'syntax']]),\n",
    "                    steps_per_epoch= train_df.shape[0]//batch_size,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs= 25,\n",
    "                    verbose = 1,\n",
    "                    shuffle= True,\n",
    "                    callbacks=[model_checkpoint_callback])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T15:48:40.505026Z",
     "iopub.execute_input": "2022-12-09T15:48:40.505388Z",
     "iopub.status.idle": "2022-12-09T22:36:02.720951Z",
     "shell.execute_reply.started": "2022-12-09T15:48:40.505356Z",
     "shell.execute_reply": "2022-12-09T22:36:02.719930Z"
    },
    "trusted": true
   },
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/25\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2022-12-09 15:48:41.642055: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "1564/1564 [==============================] - 983s 627ms/step - loss: 0.4865 - MCRMSE: 0.4865 - val_loss: 0.4670 - val_MCRMSE: 0.4670\n\nEpoch 00001: val_loss improved from inf to 0.46702, saving model to checkpoints_bert_final/\nEpoch 2/25\n1564/1564 [==============================] - 978s 625ms/step - loss: 0.4169 - MCRMSE: 0.4169 - val_loss: 0.4438 - val_MCRMSE: 0.4438\n\nEpoch 00002: val_loss improved from 0.46702 to 0.44378, saving model to checkpoints_bert_final/\nEpoch 3/25\n1564/1564 [==============================] - 980s 627ms/step - loss: 0.3651 - MCRMSE: 0.3651 - val_loss: 0.4589 - val_MCRMSE: 0.4589\n\nEpoch 00003: val_loss did not improve from 0.44378\nEpoch 4/25\n1564/1564 [==============================] - 966s 617ms/step - loss: 0.3208 - MCRMSE: 0.3208 - val_loss: 0.4464 - val_MCRMSE: 0.4464\n\nEpoch 00004: val_loss did not improve from 0.44378\nEpoch 5/25\n1564/1564 [==============================] - 964s 616ms/step - loss: 0.2879 - MCRMSE: 0.2879 - val_loss: 0.4668 - val_MCRMSE: 0.4668\n\nEpoch 00005: val_loss did not improve from 0.44378\nEpoch 6/25\n1564/1564 [==============================] - 965s 617ms/step - loss: 0.2667 - MCRMSE: 0.2667 - val_loss: 0.4507 - val_MCRMSE: 0.4507\n\nEpoch 00006: val_loss did not improve from 0.44378\nEpoch 7/25\n1564/1564 [==============================] - 965s 617ms/step - loss: 0.2455 - MCRMSE: 0.2455 - val_loss: 0.4593 - val_MCRMSE: 0.4593\n\nEpoch 00007: val_loss did not improve from 0.44378\nEpoch 8/25\n1564/1564 [==============================] - 966s 617ms/step - loss: 0.2205 - MCRMSE: 0.2205 - val_loss: 0.4509 - val_MCRMSE: 0.4509\n\nEpoch 00008: val_loss did not improve from 0.44378\nEpoch 9/25\n1564/1564 [==============================] - 968s 619ms/step - loss: 0.1997 - MCRMSE: 0.1997 - val_loss: 0.4544 - val_MCRMSE: 0.4544\n\nEpoch 00009: val_loss did not improve from 0.44378\nEpoch 10/25\n1564/1564 [==============================] - 971s 621ms/step - loss: 0.1856 - MCRMSE: 0.1856 - val_loss: 0.4584 - val_MCRMSE: 0.4584\n\nEpoch 00010: val_loss did not improve from 0.44378\nEpoch 11/25\n1564/1564 [==============================] - 972s 621ms/step - loss: 0.1673 - MCRMSE: 0.1673 - val_loss: 0.4593 - val_MCRMSE: 0.4593\n\nEpoch 00011: val_loss did not improve from 0.44378\nEpoch 12/25\n1564/1564 [==============================] - 986s 630ms/step - loss: 0.1575 - MCRMSE: 0.1575 - val_loss: 0.4558 - val_MCRMSE: 0.4558\n\nEpoch 00012: val_loss did not improve from 0.44378\nEpoch 13/25\n1564/1564 [==============================] - 986s 631ms/step - loss: 0.1500 - MCRMSE: 0.1500 - val_loss: 0.4527 - val_MCRMSE: 0.4527\n\nEpoch 00013: val_loss did not improve from 0.44378\nEpoch 14/25\n1564/1564 [==============================] - 989s 632ms/step - loss: 0.1427 - MCRMSE: 0.1427 - val_loss: 0.4514 - val_MCRMSE: 0.4514\n\nEpoch 00014: val_loss did not improve from 0.44378\nEpoch 15/25\n1564/1564 [==============================] - 989s 633ms/step - loss: 0.1384 - MCRMSE: 0.1384 - val_loss: 0.4480 - val_MCRMSE: 0.4480\n\nEpoch 00015: val_loss did not improve from 0.44378\nEpoch 16/25\n1564/1564 [==============================] - 987s 631ms/step - loss: 0.1292 - MCRMSE: 0.1292 - val_loss: 0.4579 - val_MCRMSE: 0.4579\n\nEpoch 00016: val_loss did not improve from 0.44378\nEpoch 17/25\n1564/1564 [==============================] - 988s 632ms/step - loss: 0.1283 - MCRMSE: 0.1283 - val_loss: 0.4446 - val_MCRMSE: 0.4446\n\nEpoch 00017: val_loss did not improve from 0.44378\nEpoch 18/25\n1564/1564 [==============================] - 986s 631ms/step - loss: 0.1232 - MCRMSE: 0.1232 - val_loss: 0.4494 - val_MCRMSE: 0.4494\n\nEpoch 00018: val_loss did not improve from 0.44378\nEpoch 19/25\n1564/1564 [==============================] - 971s 621ms/step - loss: 0.1184 - MCRMSE: 0.1184 - val_loss: 0.4464 - val_MCRMSE: 0.4464\n\nEpoch 00019: val_loss did not improve from 0.44378\nEpoch 20/25\n1564/1564 [==============================] - 987s 631ms/step - loss: 0.1146 - MCRMSE: 0.1146 - val_loss: 0.4510 - val_MCRMSE: 0.4510\n\nEpoch 00020: val_loss did not improve from 0.44378\nEpoch 21/25\n1564/1564 [==============================] - 968s 619ms/step - loss: 0.1136 - MCRMSE: 0.1136 - val_loss: 0.4455 - val_MCRMSE: 0.4455\n\nEpoch 00021: val_loss did not improve from 0.44378\nEpoch 22/25\n1564/1564 [==============================] - 965s 617ms/step - loss: 0.1084 - MCRMSE: 0.1084 - val_loss: 0.4464 - val_MCRMSE: 0.4464\n\nEpoch 00022: val_loss did not improve from 0.44378\nEpoch 23/25\n1564/1564 [==============================] - 978s 625ms/step - loss: 0.1053 - MCRMSE: 0.1053 - val_loss: 0.4500 - val_MCRMSE: 0.4500\n\nEpoch 00023: val_loss did not improve from 0.44378\nEpoch 24/25\n1564/1564 [==============================] - 962s 615ms/step - loss: 0.1014 - MCRMSE: 0.1014 - val_loss: 0.4452 - val_MCRMSE: 0.4452\n\nEpoch 00024: val_loss did not improve from 0.44378\nEpoch 25/25\n1564/1564 [==============================] - 978s 626ms/step - loss: 0.0973 - MCRMSE: 0.0973 - val_loss: 0.4420 - val_MCRMSE: 0.4420\n\nEpoch 00025: val_loss improved from 0.44378 to 0.44202, saving model to checkpoints_bert_final/\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(deez_nuts.history['loss'], label='loss')\n",
    "plt.plot(deez_nuts.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error [MSE]')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T22:36:02.724233Z",
     "iopub.execute_input": "2022-12-09T22:36:02.724629Z",
     "iopub.status.idle": "2022-12-09T22:36:02.995514Z",
     "shell.execute_reply.started": "2022-12-09T22:36:02.724588Z",
     "shell.execute_reply": "2022-12-09T22:36:02.994481Z"
    },
    "trusted": true
   },
   "execution_count": 43,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "json.dump(deez_nuts.history, open(\"deez_nuts.json\", 'w'))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T22:36:02.996860Z",
     "iopub.execute_input": "2022-12-09T22:36:02.997730Z",
     "iopub.status.idle": "2022-12-09T22:36:03.004774Z",
     "shell.execute_reply.started": "2022-12-09T22:36:02.997690Z",
     "shell.execute_reply": "2022-12-09T22:36:03.003901Z"
    },
    "trusted": true
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
