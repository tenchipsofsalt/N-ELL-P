{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "vscode": {
   "interpreter": {
    "hash": "5d7a279b3fb804aa2a488e25e21d217aa4e6f910ce24c7abf709fdf3600e70da"
   }
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.data import load\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "#from transformer import positional_encoding, EncoderLayer\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "from transformers import BertTokenizer , TFBertModel\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:41:51.394582Z",
     "iopub.execute_input": "2022-12-09T05:41:51.396120Z",
     "iopub.status.idle": "2022-12-09T05:42:05.491973Z",
     "shell.execute_reply.started": "2022-12-09T05:41:51.396026Z",
     "shell.execute_reply": "2022-12-09T05:42:05.490777Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/input/feedback-prize-english-language-learning/sample_submission.csv\n/kaggle/input/feedback-prize-english-language-learning/train.csv\n/kaggle/input/feedback-prize-english-language-learning/test.csv\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/config.json\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/tokenizer.json\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/tf_model.h5\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/vocab.txt\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/flax_model.msgpack\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/whole-word-masking/._bert_config.json\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/whole-word-masking/bert_config.json\n/kaggle/input/huggingface-bert-variants/bert-large-uncased/bert-large-uncased/whole-word-masking/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/384-8bits.tflite\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/tokenizer.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/tf_model.h5\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/384-fp16.tflite\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/vocab.txt\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/384.tflite\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/saved_model.pb\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/._variables\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/._assets\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/._saved_model.pb\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/._.\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/variables/variables.index\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/variables/._variables.index\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/variables/._variables.data-00000-of-00001\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/variables/variables.data-00000-of-00001\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/PaxHeader/assets\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/PaxHeader/variables\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased-distilled-squad/distilbert-base-uncased-distilled-squad/saved_model/PaxHeader/currentdir\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/config.json\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/tokenizer.json\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/tf_model.h5\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/vocab.txt\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/flax_model.msgpack\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/whole-word-masking/._bert_config.json\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/whole-word-masking/bert_config.json\n/kaggle/input/huggingface-bert-variants/bert-large-cased/bert-large-cased/whole-word-masking/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/bert-base-cased/bert-base-cased/config.json\n/kaggle/input/huggingface-bert-variants/bert-base-cased/bert-base-cased/tokenizer.json\n/kaggle/input/huggingface-bert-variants/bert-base-cased/bert-base-cased/tf_model.h5\n/kaggle/input/huggingface-bert-variants/bert-base-cased/bert-base-cased/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/bert-base-cased/bert-base-cased/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/bert-base-cased/bert-base-cased/vocab.txt\n/kaggle/input/huggingface-bert-variants/bert-base-cased/bert-base-cased/flax_model.msgpack\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased/rust_model.ot\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased/config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased/tokenizer.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased/tf_model.h5\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased/vocab.txt\n/kaggle/input/huggingface-bert-variants/distilbert-base-multilingual-cased/distilbert-base-multilingual-cased/config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-multilingual-cased/distilbert-base-multilingual-cased/tokenizer.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-multilingual-cased/distilbert-base-multilingual-cased/tf_model.h5\n/kaggle/input/huggingface-bert-variants/distilbert-base-multilingual-cased/distilbert-base-multilingual-cased/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-multilingual-cased/distilbert-base-multilingual-cased/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-multilingual-cased/distilbert-base-multilingual-cased/vocab.txt\n/kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased/rust_model.ot\n/kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased/config.json\n/kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased/tokenizer.json\n/kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased/tf_model.h5\n/kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased/vocab.txt\n/kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased/flax_model.msgpack\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/rust_model.ot\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tokenizer.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tf_model.h5\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/vocab.txt\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/saved_model/saved_model.pb\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/saved_model/variables/variables.index\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/saved_model/variables/variables.data-00000-of-00001\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard10of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard40of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard2of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard4of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard57of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard48of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard23of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard22of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard45of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard19of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard39of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard38of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard56of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard41of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard55of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard32of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard15of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard16of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard17of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard36of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard18of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard27of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard58of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard6of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard61of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard53of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard14of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard46of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard33of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard62of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard47of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard54of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard63of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard12of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard8of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard43of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard59of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard13of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard30of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard25of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard1of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard51of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard42of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard7of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard9of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard49of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/model.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard21of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard34of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard3of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard24of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard28of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard31of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard35of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard5of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard20of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard26of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard50of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard29of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard44of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard11of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard37of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard52of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased-distilled-squad/distilbert-base-cased-distilled-squad/tfjs/group1-shard60of63.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased/distilbert-base-cased/config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased/distilbert-base-cased/tokenizer.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased/distilbert-base-cased/tf_model.h5\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased/distilbert-base-cased/tokenizer_config.json\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased/distilbert-base-cased/pytorch_model.bin\n/kaggle/input/huggingface-bert-variants/distilbert-base-cased/distilbert-base-cased/vocab.txt\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2022-12-09 05:42:00.415248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 05:42:00.416409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 05:42:00.417128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 05:42:00.419028: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-09 05:42:00.419351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 05:42:00.420059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 05:42:00.420748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 05:42:04.999059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 05:42:05.000243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 05:42:05.001286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-09 05:42:05.002176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15043 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# adapted from https://www.tensorflow.org/text/tutorials/transformer\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]  # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth  # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000 ** depths)  # (1, depth)\n",
    "    angle_rads = positions * angle_rates  # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads,\n",
    "                 dff, vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(\n",
    "            vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # `x` is token-IDs shape: (batch, seq_len)\n",
    "        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`.\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:42:05.495923Z",
     "iopub.execute_input": "2022-12-09T05:42:05.496589Z",
     "iopub.status.idle": "2022-12-09T05:42:05.525379Z",
     "shell.execute_reply.started": "2022-12-09T05:42:05.496552Z",
     "shell.execute_reply": "2022-12-09T05:42:05.523609Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data_dir = \"/kaggle/input/\"\n",
    "bert_dir = \"/kaggle/input/huggingface-bert-variants/bert-base-uncased/\"\n",
    "train_df = pd.read_csv(data_dir + 'feedback-prize-english-language-learning/train.csv')\n",
    "test_df = pd.read_csv(data_dir + 'feedback-prize-english-language-learning/test.csv')\n",
    "sample_df = pd.read_csv(data_dir + 'feedback-prize-english-language-learning/sample_submission.csv')\n",
    "bert_path = bert_dir + 'bert-base-uncased'\n",
    "print(train_df.shape, test_df.shape, sample_df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T05:42:05.527029Z",
     "iopub.execute_input": "2022-12-09T05:42:05.527534Z",
     "iopub.status.idle": "2022-12-09T05:42:05.767028Z",
     "shell.execute_reply.started": "2022-12-09T05:42:05.527497Z",
     "shell.execute_reply": "2022-12-09T05:42:05.766022Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "(3911, 8) (3, 2) (3, 7)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "size = train_df.shape[0]\n",
    "train, validate = int(0.8*size), int(0.2*size)\n",
    "valid_df = train_df.tail(validate).copy()\n",
    "train_df = train_df.head(train).copy()\n",
    "print(train_df.shape, valid_df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T05:42:05.772712Z",
     "iopub.execute_input": "2022-12-09T05:42:05.774997Z",
     "iopub.status.idle": "2022-12-09T05:42:05.786426Z",
     "shell.execute_reply.started": "2022-12-09T05:42:05.774959Z",
     "shell.execute_reply": "2022-12-09T05:42:05.785376Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "(3128, 8) (782, 8)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Merging Train and Test Data\n",
    "train_size = train_df.shape[0]\n",
    "test_size = test_df.shape[0]\n",
    "print(train_df.shape, test_df.shape, valid_df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T05:42:05.790698Z",
     "iopub.execute_input": "2022-12-09T05:42:05.793437Z",
     "iopub.status.idle": "2022-12-09T05:42:05.801967Z",
     "shell.execute_reply.started": "2022-12-09T05:42:05.793399Z",
     "shell.execute_reply": "2022-12-09T05:42:05.800888Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "(3128, 8) (3, 2) (782, 8)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text) :\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+','', text)\n",
    "    text = re.sub(r'@[0-9a-zA-Z]*\\W+',' ' , text)\n",
    "\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub(r'\\#', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "\n",
    "    list_text = text.split()\n",
    "    text = ' '.join(list_text)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T05:42:05.803449Z",
     "iopub.execute_input": "2022-12-09T05:42:05.804623Z",
     "iopub.status.idle": "2022-12-09T05:42:05.815598Z",
     "shell.execute_reply.started": "2022-12-09T05:42:05.804587Z",
     "shell.execute_reply": "2022-12-09T05:42:05.814583Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "def encode(input_text):\n",
    "    inputs = tokenizer.batch_encode_plus(input_text,padding='max_length',max_length=max_text_len, truncation=True)\n",
    "    return inputs"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:42:05.816995Z",
     "iopub.execute_input": "2022-12-09T05:42:05.817584Z",
     "iopub.status.idle": "2022-12-09T05:42:05.897602Z",
     "shell.execute_reply.started": "2022-12-09T05:42:05.817550Z",
     "shell.execute_reply": "2022-12-09T05:42:05.896616Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text_vocab = set()\n",
    "pos_vocab = list(load('help/tagsets/upenn_tagset.pickle').keys())\n",
    "max_text_len = 0\n",
    "truncate_to = 512\n",
    "for dataset in [train_df, valid_df, test_df]:\n",
    "    #dataset.drop(['text_id'], axis=1, inplace=True)\n",
    "    dataset['full_text'] = dataset['full_text'].apply(lambda text : preprocess(text))\n",
    "    dataset['pos_tag'] = dataset['full_text'].apply(lambda text: pos_tag(word_tokenize(text)))\n",
    "    # there are 36 possible pos_tags\n",
    "    dataset['pos'] = dataset['pos_tag'].apply(lambda text: ' '.join([elem[1] for elem in text[:truncate_to]]))\n",
    "    dataset['tokens'] = dataset['pos_tag'].apply(lambda text: [elem[0] for elem in text[:truncate_to]])\n",
    "    for tokens in dataset['tokens']:\n",
    "        text_vocab.update(tokens)\n",
    "        max_text_len = max(max_text_len, len(tokens))\n",
    "    dataset['tokens'] = dataset['tokens'].apply(lambda text: ' '.join(text))\n",
    "#     dataset.drop(['full_text'], axis=1, inplace=True)\n",
    "    dataset.drop(['pos_tag'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T05:42:05.898879Z",
     "iopub.execute_input": "2022-12-09T05:42:05.899428Z",
     "iopub.status.idle": "2022-12-09T05:43:22.663044Z",
     "shell.execute_reply.started": "2022-12-09T05:42:05.899391Z",
     "shell.execute_reply": "2022-12-09T05:43:22.662096Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_bert = encode(train_df['full_text'].values.tolist())['input_ids']\n",
    "valid_bert = encode(valid_df['full_text'].values.tolist())['input_ids']\n",
    "test_bert = encode(test_df['full_text'].values.tolist())['input_ids']"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:43:22.664241Z",
     "iopub.execute_input": "2022-12-09T05:43:22.664572Z",
     "iopub.status.idle": "2022-12-09T05:44:06.152017Z",
     "shell.execute_reply.started": "2022-12-09T05:43:22.664538Z",
     "shell.execute_reply": "2022-12-09T05:44:06.151027Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df['bert'] = train_bert\n",
    "valid_df['bert'] = valid_bert\n",
    "test_df['bert'] = test_bert"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.156482Z",
     "iopub.execute_input": "2022-12-09T05:44:06.156785Z",
     "iopub.status.idle": "2022-12-09T05:44:06.164603Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.156757Z",
     "shell.execute_reply": "2022-12-09T05:44:06.163491Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "all_data = pd.concat((train_df, valid_df, test_df)).reset_index(drop=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.166306Z",
     "iopub.execute_input": "2022-12-09T05:44:06.166681Z",
     "iopub.status.idle": "2022-12-09T05:44:06.181231Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.166620Z",
     "shell.execute_reply": "2022-12-09T05:44:06.180173Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "max_text_len"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.183073Z",
     "iopub.execute_input": "2022-12-09T05:44:06.183436Z",
     "iopub.status.idle": "2022-12-09T05:44:06.192379Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.183401Z",
     "shell.execute_reply": "2022-12-09T05:44:06.191217Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "execution_count": 12,
     "output_type": "execute_result",
     "data": {
      "text/plain": "512"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# tag parts of speech, add as feature\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.194031Z",
     "iopub.execute_input": "2022-12-09T05:44:06.194371Z",
     "iopub.status.idle": "2022-12-09T05:44:06.222846Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.194337Z",
     "shell.execute_reply": "2022-12-09T05:44:06.221838Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "execution_count": 13,
     "output_type": "execute_result",
     "data": {
      "text/plain": "        text_id                                          full_text  cohesion  \\\n0  0016926B079C  i think that students would benefit from learn...       3.5   \n1  0022683E9EA5  when a problem is a change you have to let it ...       2.5   \n2  00299B378633  dear, principal if u change the school policy ...       3.0   \n3  003885A45F42  the best time in life is when you become yours...       4.5   \n4  0049B1DF5CCC  small act of kindness can impact in other peop...       2.5   \n\n   syntax  vocabulary  phraseology  grammar  conventions  \\\n0     3.5         3.0          3.0      4.0          3.0   \n1     2.5         3.0          2.0      2.0          2.5   \n2     3.5         3.0          3.0      3.0          2.5   \n3     4.5         4.5          4.5      4.0          5.0   \n4     3.0         3.0          3.0      2.5          2.5   \n\n                                                 pos  \\\n0  NN VBP IN NNS MD VB IN VBG IN NN , IN PRP VBP ...   \n1  WRB DT NN VBZ DT NN PRP VBP TO VB PRP VB DT JJ...   \n2  NN , JJ IN JJ VBP DT NN NN IN VBG DT NN NN NN ...   \n3  DT JJS NN IN NN VBZ WRB PRP VBP PRP . VB VBP I...   \n4  JJ NN IN NN MD VB IN JJ NNS MD VB NNS TO VB JJ...   \n\n                                              tokens  \\\n0  i think that students would benefit from learn...   \n1  when a problem is a change you have to let it ...   \n2  dear , principal if u change the school policy...   \n3  the best time in life is when you become yours...   \n4  small act of kindness can impact in other peop...   \n\n                                                bert  \n0  [101, 1045, 2228, 2008, 2493, 2052, 5770, 2013...  \n1  [101, 2043, 1037, 3291, 2003, 1037, 2689, 2017...  \n2  [101, 6203, 1010, 4054, 2065, 1057, 2689, 1996...  \n3  [101, 1996, 2190, 2051, 1999, 2166, 2003, 2043...  \n4  [101, 2235, 2552, 1997, 16056, 2064, 4254, 199...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>full_text</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n      <th>pos</th>\n      <th>tokens</th>\n      <th>bert</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0016926B079C</td>\n      <td>i think that students would benefit from learn...</td>\n      <td>3.5</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>NN VBP IN NNS MD VB IN VBG IN NN , IN PRP VBP ...</td>\n      <td>i think that students would benefit from learn...</td>\n      <td>[101, 1045, 2228, 2008, 2493, 2052, 5770, 2013...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0022683E9EA5</td>\n      <td>when a problem is a change you have to let it ...</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.5</td>\n      <td>WRB DT NN VBZ DT NN PRP VBP TO VB PRP VB DT JJ...</td>\n      <td>when a problem is a change you have to let it ...</td>\n      <td>[101, 2043, 1037, 3291, 2003, 1037, 2689, 2017...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00299B378633</td>\n      <td>dear, principal if u change the school policy ...</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>NN , JJ IN JJ VBP DT NN NN IN VBG DT NN NN NN ...</td>\n      <td>dear , principal if u change the school policy...</td>\n      <td>[101, 6203, 1010, 4054, 2065, 1057, 2689, 1996...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>003885A45F42</td>\n      <td>the best time in life is when you become yours...</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>DT JJS NN IN NN VBZ WRB PRP VBP PRP . VB VBP I...</td>\n      <td>the best time in life is when you become yours...</td>\n      <td>[101, 1996, 2190, 2051, 1999, 2166, 2003, 2043...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0049B1DF5CCC</td>\n      <td>small act of kindness can impact in other peop...</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>JJ NN IN NN MD VB IN JJ NNS MD VB NNS TO VB JJ...</td>\n      <td>small act of kindness can impact in other peop...</td>\n      <td>[101, 2235, 2552, 1997, 16056, 2064, 4254, 199...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# \"(E)lement-(Wi)se (Dense)\" Layer for combining two embeddings (or other multi-feature time sequence data) with a Dense layer applied element-wise (so not exactly Dense, as in the output embedding the first position is only determined by a linear combination of the two embedding values in corresponding positions in the two input embeddings)\n",
    "# (We picked this name because it was funny)\n",
    "class EWiDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, activation=None, **kwargs):\n",
    "        super(EWiDense, self).__init__(**kwargs)\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embedding_size = input_shape[0][-1]\n",
    "        # print(self.tile_shape)\n",
    "        self.w1 = self.add_weight(\n",
    "            shape=[self.embedding_size],\n",
    "            initializer=\"ones\",\n",
    "            trainable=True,\n",
    "            name=\"w1\"\n",
    "        )\n",
    "        # print(tf.shape(self.w1))\n",
    "        self.w2 = self.add_weight(\n",
    "            shape=[self.embedding_size],\n",
    "            initializer=\"ones\",\n",
    "            trainable=True,\n",
    "            name=\"w2\"\n",
    "        )\n",
    "        self.b1 = self.add_weight(\n",
    "            shape=[self.embedding_size],\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "            name=\"b1\"\n",
    "        )\n",
    "\n",
    "    def call(self, data):  # expected x of two embeddings of shape batch_size, seq_len, embedding_size\n",
    "        if self.activation:\n",
    "            return self.activation(tf.multiply(data[0], self.w1) + tf.multiply(data[1], self.w2) + self.b1)\n",
    "        return tf.multiply(data[0], self.w1) + tf.multiply(data[1], self.w2) + self.b1"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.224113Z",
     "iopub.execute_input": "2022-12-09T05:44:06.224419Z",
     "iopub.status.idle": "2022-12-09T05:44:06.235588Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.224388Z",
     "shell.execute_reply": "2022-12-09T05:44:06.234625Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# compare standard positional encoding with grammar + positional encodinng\n",
    "# use encoder networks, but not the decoders because we don't have an output sequence really\n",
    "\n",
    "class GrammarModel(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               max_text_len, text_vocab, pos_vocab, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.text_vectorization = tf.keras.layers.TextVectorization(output_mode='int', output_sequence_length=max_text_len)\n",
    "            self.text_vectorization.adapt(text_vocab)\n",
    "            self.pos_vectorization = tf.keras.layers.TextVectorization(output_mode='int', output_sequence_length=max_text_len)\n",
    "            self.pos_vectorization.adapt(pos_vocab)\n",
    "            self.word_embedding = tf.keras.layers.Embedding(self.text_vectorization.vocabulary_size(), d_model) # replace\n",
    "            self.pos_embedding = tf.keras.layers.Embedding(self.pos_vectorization.vocabulary_size(), d_model)\n",
    "        self.EWiDenseLayer = EWiDense(activation=tf.keras.layers.LeakyReLU())\n",
    "        self.pos_encoding = tf.Variable(positional_encoding(length=max_text_len, depth=d_model), trainable=False)\n",
    "        self.pos_scalar = tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.dense = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        words = inputs[:, 0]\n",
    "        pos = inputs[:, 1]\n",
    "        # combine embeddings\n",
    "        words = self.text_vectorization(words)\n",
    "        pos = self.pos_vectorization(pos)\n",
    "        x = self.EWiDenseLayer((self.word_embedding(words), self.pos_embedding(pos)))\n",
    "        # add positional encoding\n",
    "        x = x * self.pos_scalar\n",
    "        x = x + self.pos_encoding\n",
    "        # dropout\n",
    "        x = self.dropout(x)\n",
    "        # add encoding layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.layernorm(x)\n",
    "        return self.dense(x)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.237218Z",
     "iopub.execute_input": "2022-12-09T05:44:06.237629Z",
     "iopub.status.idle": "2022-12-09T05:44:06.250959Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.237593Z",
     "shell.execute_reply": "2022-12-09T05:44:06.249938Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class SyntaxModel(GrammarModel):\n",
    "    def __init__(self, hidden_size, *args, **kwargs):\n",
    "        super(SyntaxModel, self).__init__(*args, **kwargs)\n",
    "        self.inter_dense = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "        self.syntax_dense = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        words = inputs[:, 0]\n",
    "        pos = inputs[:, 1]\n",
    "        # combine embeddings\n",
    "        words = self.text_vectorization(words)\n",
    "        pos = self.pos_vectorization(pos)\n",
    "        x = self.EWiDenseLayer((self.word_embedding(words), self.pos_embedding(pos)))\n",
    "        # add positional encoding\n",
    "        x = x * self.pos_scalar\n",
    "        x = x + self.pos_encoding\n",
    "        # dropout\n",
    "        x = self.dropout(x)\n",
    "        # add encoding layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.layernorm(x)\n",
    "        self.encoder_out = x #(batch_size, embed_size)\n",
    "        grammar_out = self.dense(x) #(batch_size, 1)\n",
    "        \n",
    "        concatenated = tf.concat((self.encoder_out, grammar_out), axis=1)\n",
    "        x = self.inter_dense(concatenated)\n",
    "        syntax_out = self.syntax_dense(x)\n",
    "        return tf.concat((grammar_out, syntax_out), axis=1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.252506Z",
     "iopub.execute_input": "2022-12-09T05:44:06.253109Z",
     "iopub.status.idle": "2022-12-09T05:44:06.263439Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.253077Z",
     "shell.execute_reply": "2022-12-09T05:44:06.262665Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class BERTSyntaxModel(GrammarModel):\n",
    "    def __init__(self, hidden_size, *args, **kwargs):\n",
    "        super(BERTSyntaxModel, self).__init__(*args, **kwargs)\n",
    "        del self.word_embedding\n",
    "        del self.text_vectorization\n",
    "        self.inter_dense = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "        self.syntax_dense = tf.keras.layers.Dense(1)\n",
    "        self.bert_encoder = TFBertModel.from_pretrained(bert_path)\n",
    "        self.bert_down_size = tf.keras.layers.Dense(self.d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        words = inputs[0]\n",
    "        pos = inputs[1]\n",
    "        # combine embeddings\n",
    "        embedding = self.bert_encoder(words)[0]\n",
    "        embedding = self.bert_down_size(embedding)\n",
    "        pos = self.pos_vectorization(pos)\n",
    "        x = self.EWiDenseLayer((embedding, self.pos_embedding(pos)))\n",
    "        # add positional encoding\n",
    "        x = x * self.pos_scalar\n",
    "        x = x + self.pos_encoding\n",
    "        # dropout\n",
    "        x = self.dropout(x)\n",
    "        # add encoding layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.layernorm(x)\n",
    "        self.encoder_out = x #(batch_size, embed_size)\n",
    "        grammar_out = self.dense(x) #(batch_size, 1)\n",
    "        \n",
    "        concatenated = tf.concat((self.encoder_out, grammar_out), axis=1)\n",
    "        x = self.inter_dense(concatenated)\n",
    "        syntax_out = self.syntax_dense(x)\n",
    "        return tf.concat((grammar_out, syntax_out), axis=1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.264750Z",
     "iopub.execute_input": "2022-12-09T05:44:06.265410Z",
     "iopub.status.idle": "2022-12-09T05:44:06.276968Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.265377Z",
     "shell.execute_reply": "2022-12-09T05:44:06.276084Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## Column-wise RMSE\n",
    "def MCRMSE(y_true, y_pred):\n",
    "    mcrmse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n",
    "    return tf.reduce_mean(tf.sqrt(mcrmse), axis=-1, keepdims=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.278493Z",
     "iopub.execute_input": "2022-12-09T05:44:06.278877Z",
     "iopub.status.idle": "2022-12-09T05:44:06.288657Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.278840Z",
     "shell.execute_reply": "2022-12-09T05:44:06.287708Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_model():\n",
    "    num_layers = 6\n",
    "    d_model = 64\n",
    "    dff = 256\n",
    "    num_heads = 8\n",
    "    dropout_rate = 0.2\n",
    "    model = GrammarModel(num_layers, d_model, num_heads, dff, max_text_len, np.array(list(text_vocab)), np.array(list(pos_vocab)), dropout_rate)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-5, clipnorm=1), loss=MCRMSE, metrics=MCRMSE, run_eagerly=True)\n",
    "    return model"
   ],
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.290011Z",
     "iopub.execute_input": "2022-12-09T05:44:06.290551Z",
     "iopub.status.idle": "2022-12-09T05:44:06.298088Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.290514Z",
     "shell.execute_reply": "2022-12-09T05:44:06.297181Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class TransformerScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, scale_lr, hidden_size, warmup_steps):\n",
    "        self.scale_lr = tf.cast(scale_lr, tf.float32)\n",
    "        self.hidden_size = tf.cast(hidden_size, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        lr = self.scale_lr / tf.sqrt(self.hidden_size)\n",
    "        lr *= tf.minimum(1 / tf.sqrt(step), step * self.warmup_steps ** (-1.5))\n",
    "        return lr \n",
    "\n",
    "def create_syntax_model():\n",
    "    num_layers = 6\n",
    "    d_model = 64\n",
    "    dff = 256\n",
    "    num_heads = 8\n",
    "    dropout_rate = 0.2\n",
    "    model = BERTSyntaxModel(128, num_layers, d_model, num_heads, dff, max_text_len, np.array(list(text_vocab)), np.array(list(pos_vocab)), dropout_rate)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, clipnorm=1), loss=MCRMSE, metrics=MCRMSE, run_eagerly=True)\n",
    "    return model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.299436Z",
     "iopub.execute_input": "2022-12-09T05:44:06.299991Z",
     "iopub.status.idle": "2022-12-09T05:44:06.309718Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.299957Z",
     "shell.execute_reply": "2022-12-09T05:44:06.308852Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# model = create_model()\n",
    "# model.summary()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.311197Z",
     "iopub.execute_input": "2022-12-09T05:44:06.311783Z",
     "iopub.status.idle": "2022-12-09T05:44:06.325856Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.311749Z",
     "shell.execute_reply": "2022-12-09T05:44:06.324145Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# len(train_df.iloc[0][['tokens', 'pos']][0].split())"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.328124Z",
     "iopub.execute_input": "2022-12-09T05:44:06.329051Z",
     "iopub.status.idle": "2022-12-09T05:44:06.336637Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.329015Z",
     "shell.execute_reply": "2022-12-09T05:44:06.335164Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# model.text_vectorization.vocabulary_size()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.338130Z",
     "iopub.execute_input": "2022-12-09T05:44:06.339200Z",
     "iopub.status.idle": "2022-12-09T05:44:06.345490Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.339164Z",
     "shell.execute_reply": "2022-12-09T05:44:06.344072Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "max_text_len"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.347067Z",
     "iopub.execute_input": "2022-12-09T05:44:06.347974Z",
     "iopub.status.idle": "2022-12-09T05:44:06.360124Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.347928Z",
     "shell.execute_reply": "2022-12-09T05:44:06.355902Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": [
    {
     "execution_count": 24,
     "output_type": "execute_result",
     "data": {
      "text/plain": "512"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# model(tf.expand_dims(tf.convert_to_tensor(train_df.iloc[0][['tokens', 'pos']]), 0))\n",
    "# model.summary()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.364521Z",
     "iopub.execute_input": "2022-12-09T05:44:06.365876Z",
     "iopub.status.idle": "2022-12-09T05:44:06.370019Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.365817Z",
     "shell.execute_reply": "2022-12-09T05:44:06.368952Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# tf.debugging.disable_traceback_filtering()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.373353Z",
     "iopub.execute_input": "2022-12-09T05:44:06.374145Z",
     "iopub.status.idle": "2022-12-09T05:44:06.382182Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.374101Z",
     "shell.execute_reply": "2022-12-09T05:44:06.380267Z"
    },
    "trusted": true
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# import tensorflow as tf\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.383473Z",
     "iopub.execute_input": "2022-12-09T05:44:06.384132Z",
     "iopub.status.idle": "2022-12-09T05:44:06.388869Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.384090Z",
     "shell.execute_reply": "2022-12-09T05:44:06.387552Z"
    },
    "trusted": true
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# checkpoint_filepath = 'tmp/checkpoint'\n",
    "# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=checkpoint_filepath,\n",
    "#     monitor=\"val_loss\",\n",
    "#     verbose=1,\n",
    "#     save_weights_only=True,\n",
    "#     mode='min',\n",
    "#     save_best_only=True)\n",
    "# history = model.fit(\n",
    "#                     train_df[['tokens', 'pos']],\n",
    "#                     train_df['grammar'],\n",
    "#                     validation_data = (valid_df[['tokens', 'pos']], valid_df['grammar']),\n",
    "#                     steps_per_epoch= train_df.shape[0]//4,\n",
    "#                     batch_size = 4,\n",
    "#                     epochs= 100,\n",
    "#                     verbose = 1,\n",
    "#                     shuffle= True,\n",
    "#                     callbacks=[model_checkpoint_callback])"
   ],
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.398334Z",
     "iopub.execute_input": "2022-12-09T05:44:06.398935Z",
     "iopub.status.idle": "2022-12-09T05:44:06.404195Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.398900Z",
     "shell.execute_reply": "2022-12-09T05:44:06.402755Z"
    },
    "trusted": true
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# history"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.405798Z",
     "iopub.execute_input": "2022-12-09T05:44:06.406525Z",
     "iopub.status.idle": "2022-12-09T05:44:06.417283Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.406482Z",
     "shell.execute_reply": "2022-12-09T05:44:06.416200Z"
    },
    "trusted": true
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# model(tf.expand_dims(tf.convert_to_tensor(train_df.iloc[0][['tokens', 'pos']]), 0))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.418571Z",
     "iopub.execute_input": "2022-12-09T05:44:06.419235Z",
     "iopub.status.idle": "2022-12-09T05:44:06.425740Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.419199Z",
     "shell.execute_reply": "2022-12-09T05:44:06.424705Z"
    },
    "trusted": true
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "syntax_model = create_syntax_model()"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:06.427184Z",
     "iopub.execute_input": "2022-12-09T05:44:06.429063Z",
     "iopub.status.idle": "2022-12-09T05:44:14.380544Z",
     "shell.execute_reply.started": "2022-12-09T05:44:06.428978Z",
     "shell.execute_reply": "2022-12-09T05:44:14.379688Z"
    },
    "trusted": true
   },
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "text": "2022-12-09 05:44:06.590440: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\nSome layers from the model checkpoint at /kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at /kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# test = (np.array(train_df['bert'].values.tolist()), train_df['pos'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:14.382623Z",
     "iopub.execute_input": "2022-12-09T05:44:14.383406Z",
     "iopub.status.idle": "2022-12-09T05:44:14.388093Z",
     "shell.execute_reply.started": "2022-12-09T05:44:14.383368Z",
     "shell.execute_reply": "2022-12-09T05:44:14.386876Z"
    },
    "trusted": true
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# test[1].shape"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:14.390156Z",
     "iopub.execute_input": "2022-12-09T05:44:14.390616Z",
     "iopub.status.idle": "2022-12-09T05:44:14.398737Z",
     "shell.execute_reply.started": "2022-12-09T05:44:14.390578Z",
     "shell.execute_reply": "2022-12-09T05:44:14.397667Z"
    },
    "trusted": true
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# syntax_model(test)\n",
    "# syntax_model.summary()"
   ],
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:14.401379Z",
     "iopub.execute_input": "2022-12-09T05:44:14.401989Z",
     "iopub.status.idle": "2022-12-09T05:44:14.409640Z",
     "shell.execute_reply.started": "2022-12-09T05:44:14.401960Z",
     "shell.execute_reply": "2022-12-09T05:44:14.408677Z"
    },
    "trusted": true
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 2\n",
    "checkpoint_filepath = 'checkpoints_bert_final/'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "deez_nuts = syntax_model.fit(\n",
    "                    (np.array(train_df['bert'].values.tolist()), train_df['pos']),\n",
    "                    train_df[['grammar', 'syntax']],\n",
    "                    validation_data = ((np.array(valid_df['bert'].values.tolist()), valid_df['pos']), valid_df[['grammar', 'syntax']]),\n",
    "                    steps_per_epoch= train_df.shape[0]//batch_size,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs= 25,\n",
    "                    verbose = 1,\n",
    "                    shuffle= True,\n",
    "                    callbacks=[model_checkpoint_callback])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T05:44:14.411233Z",
     "iopub.execute_input": "2022-12-09T05:44:14.411622Z",
     "iopub.status.idle": "2022-12-09T12:22:36.625525Z",
     "shell.execute_reply.started": "2022-12-09T05:44:14.411588Z",
     "shell.execute_reply": "2022-12-09T12:22:36.624425Z"
    },
    "trusted": true
   },
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/25\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2022-12-09 05:44:15.578911: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "1564/1564 [==============================] - 960s 613ms/step - loss: 0.4868 - MCRMSE: 0.4868 - val_loss: 0.4463 - val_MCRMSE: 0.4463\n\nEpoch 00001: val_loss improved from inf to 0.44632, saving model to checkpoints_bert_final/\nEpoch 2/25\n1564/1564 [==============================] - 956s 611ms/step - loss: 0.4208 - MCRMSE: 0.4208 - val_loss: 0.4400 - val_MCRMSE: 0.4400\n\nEpoch 00002: val_loss improved from 0.44632 to 0.44003, saving model to checkpoints_bert_final/\nEpoch 3/25\n1564/1564 [==============================] - 958s 612ms/step - loss: 0.3746 - MCRMSE: 0.3746 - val_loss: 0.4639 - val_MCRMSE: 0.4639\n\nEpoch 00003: val_loss did not improve from 0.44003\nEpoch 4/25\n1564/1564 [==============================] - 941s 602ms/step - loss: 0.3277 - MCRMSE: 0.3277 - val_loss: 0.4877 - val_MCRMSE: 0.4877\n\nEpoch 00004: val_loss did not improve from 0.44003\nEpoch 5/25\n1564/1564 [==============================] - 961s 614ms/step - loss: 0.2955 - MCRMSE: 0.2955 - val_loss: 0.4970 - val_MCRMSE: 0.4970\n\nEpoch 00005: val_loss did not improve from 0.44003\nEpoch 6/25\n1564/1564 [==============================] - 941s 602ms/step - loss: 0.2717 - MCRMSE: 0.2717 - val_loss: 0.4487 - val_MCRMSE: 0.4487\n\nEpoch 00006: val_loss did not improve from 0.44003\nEpoch 7/25\n1564/1564 [==============================] - 944s 603ms/step - loss: 0.2536 - MCRMSE: 0.2536 - val_loss: 0.4484 - val_MCRMSE: 0.4484\n\nEpoch 00007: val_loss did not improve from 0.44003\nEpoch 8/25\n1564/1564 [==============================] - 964s 617ms/step - loss: 0.2280 - MCRMSE: 0.2280 - val_loss: 0.4798 - val_MCRMSE: 0.4798\n\nEpoch 00008: val_loss did not improve from 0.44003\nEpoch 9/25\n1564/1564 [==============================] - 963s 615ms/step - loss: 0.2043 - MCRMSE: 0.2043 - val_loss: 0.4791 - val_MCRMSE: 0.4791\n\nEpoch 00009: val_loss did not improve from 0.44003\nEpoch 10/25\n1564/1564 [==============================] - 962s 615ms/step - loss: 0.1877 - MCRMSE: 0.1877 - val_loss: 0.4804 - val_MCRMSE: 0.4804\n\nEpoch 00010: val_loss did not improve from 0.44003\nEpoch 11/25\n1564/1564 [==============================] - 950s 607ms/step - loss: 0.1771 - MCRMSE: 0.1771 - val_loss: 0.4861 - val_MCRMSE: 0.4861\n\nEpoch 00011: val_loss did not improve from 0.44003\nEpoch 12/25\n1564/1564 [==============================] - 975s 624ms/step - loss: 0.1644 - MCRMSE: 0.1644 - val_loss: 0.4538 - val_MCRMSE: 0.4538\n\nEpoch 00012: val_loss did not improve from 0.44003\nEpoch 13/25\n1564/1564 [==============================] - 951s 608ms/step - loss: 0.1550 - MCRMSE: 0.1550 - val_loss: 0.4685 - val_MCRMSE: 0.4685\n\nEpoch 00013: val_loss did not improve from 0.44003\nEpoch 14/25\n1564/1564 [==============================] - 945s 604ms/step - loss: 0.1462 - MCRMSE: 0.1462 - val_loss: 0.4769 - val_MCRMSE: 0.4769\n\nEpoch 00014: val_loss did not improve from 0.44003\nEpoch 15/25\n1564/1564 [==============================] - 944s 604ms/step - loss: 0.1429 - MCRMSE: 0.1429 - val_loss: 0.4742 - val_MCRMSE: 0.4742\n\nEpoch 00015: val_loss did not improve from 0.44003\nEpoch 16/25\n1564/1564 [==============================] - 964s 617ms/step - loss: 0.1342 - MCRMSE: 0.1342 - val_loss: 0.4704 - val_MCRMSE: 0.4704\n\nEpoch 00016: val_loss did not improve from 0.44003\nEpoch 17/25\n1564/1564 [==============================] - 945s 604ms/step - loss: 0.1324 - MCRMSE: 0.1324 - val_loss: 0.4740 - val_MCRMSE: 0.4740\n\nEpoch 00017: val_loss did not improve from 0.44003\nEpoch 18/25\n1564/1564 [==============================] - 963s 615ms/step - loss: 0.1249 - MCRMSE: 0.1249 - val_loss: 0.4535 - val_MCRMSE: 0.4535\n\nEpoch 00018: val_loss did not improve from 0.44003\nEpoch 19/25\n1564/1564 [==============================] - 945s 604ms/step - loss: 0.1214 - MCRMSE: 0.1214 - val_loss: 0.4540 - val_MCRMSE: 0.4540\n\nEpoch 00019: val_loss did not improve from 0.44003\nEpoch 20/25\n1564/1564 [==============================] - 943s 603ms/step - loss: 0.1215 - MCRMSE: 0.1215 - val_loss: 0.4671 - val_MCRMSE: 0.4671\n\nEpoch 00020: val_loss did not improve from 0.44003\nEpoch 21/25\n1564/1564 [==============================] - 963s 616ms/step - loss: 0.1162 - MCRMSE: 0.1162 - val_loss: 0.4661 - val_MCRMSE: 0.4661\n\nEpoch 00021: val_loss did not improve from 0.44003\nEpoch 22/25\n1564/1564 [==============================] - 944s 604ms/step - loss: 0.1146 - MCRMSE: 0.1146 - val_loss: 0.4551 - val_MCRMSE: 0.4551\n\nEpoch 00022: val_loss did not improve from 0.44003\nEpoch 23/25\n1564/1564 [==============================] - 945s 604ms/step - loss: 0.1097 - MCRMSE: 0.1097 - val_loss: 0.4593 - val_MCRMSE: 0.4593\n\nEpoch 00023: val_loss did not improve from 0.44003\nEpoch 24/25\n1564/1564 [==============================] - 965s 617ms/step - loss: 0.1089 - MCRMSE: 0.1089 - val_loss: 0.4621 - val_MCRMSE: 0.4621\n\nEpoch 00024: val_loss did not improve from 0.44003\nEpoch 25/25\n1564/1564 [==============================] - 964s 617ms/step - loss: 0.1059 - MCRMSE: 0.1059 - val_loss: 0.4653 - val_MCRMSE: 0.4653\n\nEpoch 00025: val_loss did not improve from 0.44003\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(deez_nuts.history['loss'], label='loss')\n",
    "plt.plot(deez_nuts.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error [MSE]')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T12:22:36.627609Z",
     "iopub.execute_input": "2022-12-09T12:22:36.627991Z",
     "iopub.status.idle": "2022-12-09T12:22:36.895659Z",
     "shell.execute_reply.started": "2022-12-09T12:22:36.627953Z",
     "shell.execute_reply": "2022-12-09T12:22:36.894742Z"
    },
    "trusted": true
   },
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "json.dump(deez_nuts.history, open(\"deez_nuts.json\", 'w'))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T12:22:36.897159Z",
     "iopub.execute_input": "2022-12-09T12:22:36.897487Z",
     "iopub.status.idle": "2022-12-09T12:22:36.903087Z",
     "shell.execute_reply.started": "2022-12-09T12:22:36.897452Z",
     "shell.execute_reply": "2022-12-09T12:22:36.902028Z"
    },
    "trusted": true
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
