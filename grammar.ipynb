{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformer import positional_encoding, EncoderLayer\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from transformers import BertTokenizer , TFBertModel\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3911, 8) (3, 2) (3, 7)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"kaggle/input/\"\n",
    "bert_dir = \"kaggle/input/huggingface-bert-variants/bert-base-uncased/\"\n",
    "train_df = pd.read_csv(data_dir + 'feedback-prize-english-language-learning/train.csv')\n",
    "test_df = pd.read_csv(data_dir + 'feedback-prize-english-language-learning/test.csv')\n",
    "sample_df = pd.read_csv(data_dir + 'feedback-prize-english-language-learning/sample_submission.csv')\n",
    "bert_path = bert_dir + 'bert-base-uncased'\n",
    "print(train_df.shape, test_df.shape, sample_df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3128, 8) (782, 8)\n"
     ]
    }
   ],
   "source": [
    "size = train_df.shape[0]\n",
    "train, validate = int(0.8*size), int(0.2*size)\n",
    "validate_df = train_df.tail(validate).copy()\n",
    "train_df = train_df.head(train).copy()\n",
    "print(train_df.shape, validate_df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3131, 7) (782, 7)\n"
     ]
    }
   ],
   "source": [
    "# Merging Train and Test Data\n",
    "train_size = train_df.shape[0]\n",
    "test_size = test_df.shape[0]\n",
    "all_data = pd.concat((train_df, test_df)).reset_index(drop=True)\n",
    "all_data.drop(['text_id'], axis=1, inplace=True)\n",
    "validate_df.drop(['text_id'], axis=1, inplace=True)\n",
    "print(all_data.shape, validate_df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text) :\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+','', text)\n",
    "    text = re.sub(r'@[0-9a-zA-Z]*\\W+',' ' , text)\n",
    "\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub(r'\\#', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "\n",
    "    list_text = text.split()\n",
    "    text = ' '.join(list_text[:512])\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "all_data['full_text'] = all_data['full_text'].apply(lambda text : preprocess(text))\n",
    "validate_df['full_text'] = validate_df['full_text'].apply(lambda text : preprocess(text))\n",
    "all_data['pos_tag'] = all_data['full_text'].apply(lambda text: pos_tag(word_tokenize(text)))\n",
    "validate_df['pos_tag'] = validate_df['full_text'].apply(lambda text: pos_tag(word_tokenize(text)))\n",
    "# there are 36 possible pos_tags, we will later encode these in one-hot representation\n",
    "all_data['pos_tag'] = all_data['pos_tag'].apply(lambda elem: elem[1])\n",
    "validate_df['pos_tag'] = validate_df['pos_tag'].apply(lambda elem: elem[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3128, 7) (3, 7)\n"
     ]
    }
   ],
   "source": [
    "train_data = all_data[:train_size].copy()\n",
    "test_data = all_data[train_size:].copy()\n",
    "\n",
    "print(train_data.shape, test_data.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "0       [(i, NN), (think, VBP), (that, IN), (students,...\n1       [(when, WRB), (a, DT), (problem, NN), (is, VBZ...\n2       [(dear, NN), (,, ,), (principal, JJ), (if, IN)...\n3       [(the, DT), (best, JJS), (time, NN), (in, IN),...\n4       [(small, JJ), (act, NN), (of, IN), (kindness, ...\n                              ...                        \n3123    [(author, NN), (ralph, NN), (waldo, NN), (emer...\n3124    [(we, PRP), (humans, NNS), (aren, VBP), (t, JJ...\n3125    [(``, ``), (your, PRP$), (character, NN), (wil...\n3126    [(the, DT), (school, NN), (board, NN), (plans,...\n3127    [(is, VBZ), (good, JJ), (idea, NN), (for, IN),...\nName: pos_tag, Length: 3128, dtype: object"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tag parts of speech, add as feature\n",
    "train_data['pos_tag']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# \"(E)lement-(Wi)se (Dense)\" Layer for combining two embeddings (or other multi-feature time sequence data) with a Dense layer applied element-wise (so not exactly Dense, as in the output embedding the first position is only determined by a linear combination of the two embedding values in corresponding positions in the two input embeddings\n",
    "class EWiDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_size, activation=None):\n",
    "        super(EWiDense, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w1 = tf.Variable(\n",
    "            initial_value=w_init(shape=(1, 1, embedding_size), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.w2 = tf.Variable(\n",
    "            initial_value=w_init(shape=(1, 1, embedding_size), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b1 = tf.Variable(\n",
    "            initial_value=b_init(shape=(1, 1, embedding_size), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "        self.b2 = tf.Variable(\n",
    "            initial_value=b_init(shape=(1, 1, embedding_size), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "        self.activation = activation\n",
    "\n",
    "    def call(self, data):  # expected x of two embeddings of shape batch_size, seq_len, embedding_size\n",
    "        emb1, emb2 = data\n",
    "        emb1 = tf.multiply(emb1, self.w1) + self.b1\n",
    "        emb2 = tf.multiply(emb2, self.w2) + self.b2\n",
    "        out = emb1 + emb2\n",
    "        if self.activation:\n",
    "            self.activation(out)\n",
    "        return out\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compare standard positional encoding with grammar + positional encodinng\n",
    "# use encoder networks, but not the decoders because we don't have an output sequence really\n",
    "\n",
    "class GrammarModel(tf.keras.Model):\n",
    "    def __init__(self, embedding_size, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.word_embedding = tf.keras.layers.Embedding(input_vocab_size, d_model) # replace with bert in the future for testing\n",
    "        self.pos_embedding = tf.keras.layers.Embedding(36, d_model)\n",
    "        self.EWiDenseLayer = EWiDense(embedding_size, activation=tf.keras.layers.LeakyReLU())\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.output = tf.keras.layers.Dense()\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        words, pos = inputs\n",
    "        length = tf.shape(words)[1] # seq_len\n",
    "        # combine embeddings\n",
    "        x = self.EwiDenseLayer((self.word_embedding(words), self.pos_embedding(pos)))\n",
    "        # add positional encoding\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        # dropout\n",
    "        x = self.dropout(x)\n",
    "        # add encoding layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.layernorm(x)\n",
    "        return self.output(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
