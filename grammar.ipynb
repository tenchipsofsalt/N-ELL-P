{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.data import load\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformer import positional_encoding, EncoderLayer\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from transformers import BertTokenizer , TFBertModel\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3911, 8) (3, 2) (3, 7)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"kaggle/input/\"\n",
    "bert_dir = \"kaggle/input/huggingface-bert-variants/bert-base-uncased/\"\n",
    "train_df = pd.read_csv(data_dir + 'feedback-prize-english-language-learning/train.csv')\n",
    "test_df = pd.read_csv(data_dir + 'feedback-prize-english-language-learning/test.csv')\n",
    "sample_df = pd.read_csv(data_dir + 'feedback-prize-english-language-learning/sample_submission.csv')\n",
    "bert_path = bert_dir + 'bert-base-uncased'\n",
    "print(train_df.shape, test_df.shape, sample_df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3128, 8) (782, 8)\n"
     ]
    }
   ],
   "source": [
    "size = train_df.shape[0]\n",
    "train, validate = int(0.8*size), int(0.2*size)\n",
    "valid_df = train_df.tail(validate).copy()\n",
    "train_df = train_df.head(train).copy()\n",
    "print(train_df.shape, valid_df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3128, 8) (3, 2) (782, 8)\n"
     ]
    }
   ],
   "source": [
    "# Merging Train and Test Data\n",
    "train_size = train_df.shape[0]\n",
    "test_size = test_df.shape[0]\n",
    "print(train_df.shape, test_df.shape, valid_df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text) :\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+','', text)\n",
    "    text = re.sub(r'@[0-9a-zA-Z]*\\W+',' ' , text)\n",
    "\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub(r'\\#', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "\n",
    "    list_text = text.split()\n",
    "    text = ' '.join(list_text)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "text_vocab = set()\n",
    "pos_vocab = list(load('help/tagsets/upenn_tagset.pickle').keys())\n",
    "max_text_len = 0\n",
    "truncate_to = 512\n",
    "for dataset in [train_df, valid_df, test_df]:\n",
    "    dataset.drop(['text_id'], axis=1, inplace=True)\n",
    "    dataset['full_text'] = dataset['full_text'].apply(lambda text : preprocess(text))\n",
    "    dataset['pos_tag'] = dataset['full_text'].apply(lambda text: pos_tag(word_tokenize(text)))\n",
    "    # there are 36 possible pos_tags\n",
    "    dataset['pos'] = dataset['pos_tag'].apply(lambda text: ' '.join([elem[1] for elem in text[:truncate_to]]))\n",
    "    dataset['tokens'] = dataset['pos_tag'].apply(lambda text: [elem[0] for elem in text[:truncate_to]])\n",
    "    for tokens in dataset['tokens']:\n",
    "        text_vocab.update(tokens)\n",
    "        max_text_len = max(max_text_len, len(tokens))\n",
    "    dataset['tokens'] = dataset['tokens'].apply(lambda text: ' '.join(text))\n",
    "    dataset.drop(['full_text'], axis=1, inplace=True)\n",
    "    dataset.drop(['pos_tag'], axis=1, inplace=True)\n",
    "all_data = pd.concat((train_df, valid_df, test_df)).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "512"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_text_len"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "   cohesion  syntax  vocabulary  phraseology  grammar  conventions  \\\n0       3.5     3.5         3.0          3.0      4.0          3.0   \n1       2.5     2.5         3.0          2.0      2.0          2.5   \n2       3.0     3.5         3.0          3.0      3.0          2.5   \n3       4.5     4.5         4.5          4.5      4.0          5.0   \n4       2.5     3.0         3.0          3.0      2.5          2.5   \n\n                                                 pos  \\\n0  NN VBP IN NNS MD VB IN VBG IN NN , IN PRP VBP ...   \n1  WRB DT NN VBZ DT NN PRP VBP TO VB PRP VB DT JJ...   \n2  NN , JJ IN JJ VBP DT NN NN IN VBG DT NN NN NN ...   \n3  DT JJS NN IN NN VBZ WRB PRP VBP PRP . VB VBP I...   \n4  JJ NN IN NN MD VB IN JJ NNS MD VB NNS TO VB JJ...   \n\n                                              tokens  \n0  i think that students would benefit from learn...  \n1  when a problem is a change you have to let it ...  \n2  dear , principal if u change the school policy...  \n3  the best time in life is when you become yours...  \n4  small act of kindness can impact in other peop...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n      <th>pos</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.5</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>NN VBP IN NNS MD VB IN VBG IN NN , IN PRP VBP ...</td>\n      <td>i think that students would benefit from learn...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.5</td>\n      <td>WRB DT NN VBZ DT NN PRP VBP TO VB PRP VB DT JJ...</td>\n      <td>when a problem is a change you have to let it ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>NN , JJ IN JJ VBP DT NN NN IN VBG DT NN NN NN ...</td>\n      <td>dear , principal if u change the school policy...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>DT JJS NN IN NN VBZ WRB PRP VBP PRP . VB VBP I...</td>\n      <td>the best time in life is when you become yours...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>JJ NN IN NN MD VB IN JJ NNS MD VB NNS TO VB JJ...</td>\n      <td>small act of kindness can impact in other peop...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tag parts of speech, add as feature\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# \"(E)lement-(Wi)se (Dense)\" Layer for combining two embeddings (or other multi-feature time sequence data) with a Dense layer applied element-wise (so not exactly Dense, as in the output embedding the first position is only determined by a linear combination of the two embedding values in corresponding positions in the two input embeddings)\n",
    "# (We picked this name because it was funny)\n",
    "class EWiDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, activation=None, **kwargs):\n",
    "        super(EWiDense, self).__init__(**kwargs)\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embedding_size = input_shape[0][-1]\n",
    "        # print(self.tile_shape)\n",
    "        self.w1 = self.add_weight(\n",
    "            shape=[self.embedding_size],\n",
    "            initializer=\"ones\",\n",
    "            trainable=True\n",
    "        )\n",
    "        # print(tf.shape(self.w1))\n",
    "        self.w2 = self.add_weight(\n",
    "            shape=[self.embedding_size],\n",
    "            initializer=\"ones\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b1 = self.add_weight(\n",
    "            shape=[self.embedding_size],\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, data):  # expected x of two embeddings of shape batch_size, seq_len, embedding_size\n",
    "        if self.activation:\n",
    "            return self.activation(tf.multiply(data[0], self.w1) + tf.multiply(data[1], self.w2) + self.b1)\n",
    "        return tf.multiply(data[0], self.w1) + tf.multiply(data[1], self.w2) + self.b1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# compare standard positional encoding with grammar + positional encodinng\n",
    "# use encoder networks, but not the decoders because we don't have an output sequence really\n",
    "\n",
    "class GrammarModel(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               max_text_len, text_vocab, pos_vocab, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.text_vectorization = tf.keras.layers.TextVectorization(output_mode='int', output_sequence_length=max_text_len)\n",
    "            self.text_vectorization.adapt(text_vocab)\n",
    "            self.pos_vectorization = tf.keras.layers.TextVectorization(output_mode='int', output_sequence_length=max_text_len)\n",
    "            self.pos_vectorization.adapt(pos_vocab)\n",
    "            self.word_embedding = tf.keras.layers.Embedding(self.text_vectorization.vocabulary_size(), d_model) # replace\n",
    "            self.pos_embedding = tf.keras.layers.Embedding(self.pos_vectorization.vocabulary_size(), d_model)\n",
    "        self.EWiDenseLayer = EWiDense(activation=tf.keras.layers.LeakyReLU())\n",
    "        self.pos_encoding = tf.Variable(positional_encoding(length=max_text_len, depth=d_model), trainable=False)\n",
    "        self.pos_scalar = tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.dense = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        words = inputs[:, 0]\n",
    "        pos = inputs[:, 1]\n",
    "        # combine embeddings\n",
    "        words = self.text_vectorization(words)\n",
    "        pos = self.pos_vectorization(pos)\n",
    "        x = self.EWiDenseLayer((self.word_embedding(words), self.pos_embedding(pos)))\n",
    "        # add positional encoding\n",
    "        x = x * self.pos_scalar\n",
    "        x = x + self.pos_encoding\n",
    "        # dropout\n",
    "        x = self.dropout(x)\n",
    "        # add encoding layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.layernorm(x)\n",
    "        return self.dense(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "## Column-wise RMSE\n",
    "def MCRMSE(y_true, y_pred):\n",
    "    mcrmse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n",
    "    return tf.reduce_mean(tf.sqrt(mcrmse), axis=-1, keepdims=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    num_layers = 6\n",
    "    d_model = 64\n",
    "    dff = 256\n",
    "    num_heads = 8\n",
    "    dropout_rate = 0.2\n",
    "    model = GrammarModel(num_layers, d_model, num_heads, dff, max_text_len, np.array(list(text_vocab)), np.array(list(pos_vocab)), dropout_rate)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-5, clipnorm=1), loss=MCRMSE, metrics=MCRMSE, run_eagerly=True)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "# model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "283"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.iloc[0][['tokens', 'pos']][0].split())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "20133"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.text_vectorization.vocabulary_size()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "512"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_text_len"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"grammar_model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_16 (Text  multiple                 0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " text_vectorization_17 (Text  multiple                 0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_16 (Embedding)    multiple                  1307392   \n",
      "                                                                 \n",
      " embedding_17 (Embedding)    multiple                  2304      \n",
      "                                                                 \n",
      " e_wi_dense_8 (EWiDense)     multiple                  192       \n",
      "                                                                 \n",
      " encoder_layer_28 (EncoderLa  multiple                 166016    \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " encoder_layer_29 (EncoderLa  multiple                 166016    \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " encoder_layer_30 (EncoderLa  multiple                 166016    \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " encoder_layer_31 (EncoderLa  multiple                 166016    \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " encoder_layer_32 (EncoderLa  multiple                 166016    \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " encoder_layer_33 (EncoderLa  multiple                 166016    \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " dropout_66 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " global_average_pooling1d_8   multiple                 0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " layer_normalization_76 (Lay  multiple                 128       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_76 (Dense)            multiple                  65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,345,537\n",
      "Trainable params: 2,306,177\n",
      "Non-trainable params: 39,360\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model(tf.expand_dims(tf.convert_to_tensor(train_df.iloc[0][['tokens', 'pos']]), 0))\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "# tf.debugging.disable_traceback_filtering()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 73/782 [=>............................] - ETA: 2:13 - loss: 1.2985 - MCRMSE: 1.2985"
     ]
    }
   ],
   "source": [
    "checkpoint_filepath = 'tmp/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_weights_only=False,\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "history = model.fit(\n",
    "                    train_df[['tokens', 'pos']],\n",
    "                    train_df['grammar'],\n",
    "                    validation_data = (valid_df[['tokens', 'pos']], valid_df['grammar']),\n",
    "                    steps_per_epoch= train_df.shape[0]//4,\n",
    "                    batch_size = 4,\n",
    "                    epochs= 100,\n",
    "                    verbose = 1,\n",
    "                    shuffle= True,\n",
    "                    callbacks=[model_checkpoint_callback])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1d5a544d870>"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "{'loss': [0.6319288015365601,\n  0.5693314671516418,\n  0.5644047260284424,\n  0.563819169998169,\n  0.5583122372627258,\n  0.5539228916168213,\n  0.5575403571128845,\n  0.5539509654045105,\n  0.5454583168029785,\n  0.5210098028182983,\n  0.5034996271133423,\n  0.4937760829925537,\n  0.4846763014793396,\n  0.48725461959838867,\n  0.47956550121307373,\n  0.47537827491760254,\n  0.4688221216201782,\n  0.4692607522010803,\n  0.46288731694221497,\n  0.45620232820510864,\n  0.457209974527359,\n  0.4546971917152405,\n  0.45028936862945557,\n  0.4479743540287018,\n  0.44166314601898193,\n  0.4422048330307007,\n  0.4352353811264038,\n  0.4331895411014557,\n  0.42566388845443726,\n  0.42861512303352356,\n  0.4187238812446594,\n  0.42049524188041687,\n  0.41448888182640076,\n  0.4155365526676178,\n  0.4093209505081177,\n  0.40732190012931824,\n  0.4102404713630676,\n  0.3964894413948059,\n  0.396979957818985,\n  0.3952256143093109,\n  0.3929370641708374,\n  0.3880009055137634,\n  0.38459107279777527,\n  0.37893083691596985,\n  0.38296860456466675,\n  0.3763081729412079,\n  0.3697524666786194,\n  0.36811837553977966,\n  0.37012210488319397,\n  0.3643684387207031,\n  0.35726889967918396,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan],\n 'MCRMSE': [0.6319288015365601,\n  0.5693314671516418,\n  0.5644047260284424,\n  0.563819169998169,\n  0.5583122372627258,\n  0.5539228916168213,\n  0.5575403571128845,\n  0.5539509654045105,\n  0.5454583168029785,\n  0.5210098028182983,\n  0.5034996271133423,\n  0.4937760829925537,\n  0.4846763014793396,\n  0.48725461959838867,\n  0.47956550121307373,\n  0.47537827491760254,\n  0.4688221216201782,\n  0.4692607522010803,\n  0.46288731694221497,\n  0.45620232820510864,\n  0.457209974527359,\n  0.4546971917152405,\n  0.45028936862945557,\n  0.4479743540287018,\n  0.44166314601898193,\n  0.4422048330307007,\n  0.4352353811264038,\n  0.4331895411014557,\n  0.42566388845443726,\n  0.42861512303352356,\n  0.4187238812446594,\n  0.42049524188041687,\n  0.41448888182640076,\n  0.4155365526676178,\n  0.4093209505081177,\n  0.40732190012931824,\n  0.4102404713630676,\n  0.3964894413948059,\n  0.396979957818985,\n  0.3952256143093109,\n  0.3929370641708374,\n  0.3880009055137634,\n  0.38459107279777527,\n  0.37893083691596985,\n  0.38296860456466675,\n  0.3763081729412079,\n  0.3697524666786194,\n  0.36811837553977966,\n  0.37012210488319397,\n  0.3643684387207031,\n  0.35726889967918396,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan],\n 'val_loss': [0.6076929569244385,\n  0.5812643766403198,\n  0.5739837884902954,\n  0.5777063369750977,\n  0.5665468573570251,\n  0.5607281923294067,\n  0.568393349647522,\n  0.5541412830352783,\n  0.6370046734809875,\n  0.5788875818252563,\n  0.5064083933830261,\n  0.5892164707183838,\n  0.5120611190795898,\n  0.5910364389419556,\n  0.513302206993103,\n  0.5105162262916565,\n  0.5100187063217163,\n  0.532493531703949,\n  0.5262150764465332,\n  0.5232210159301758,\n  0.49886295199394226,\n  0.505679726600647,\n  0.4883408546447754,\n  0.4871619641780853,\n  0.49352335929870605,\n  0.490486204624176,\n  0.4876345992088318,\n  0.5525355935096741,\n  0.5487945079803467,\n  0.5092173218727112,\n  0.5390674471855164,\n  0.5024564266204834,\n  0.48440074920654297,\n  0.5749319195747375,\n  0.48231643438339233,\n  0.5573363900184631,\n  0.5284225344657898,\n  0.5671730041503906,\n  0.48006635904312134,\n  0.4796087443828583,\n  0.49171027541160583,\n  0.5638622641563416,\n  0.4838513135910034,\n  0.5032694339752197,\n  0.4865511655807495,\n  0.527871310710907,\n  0.4817954897880554,\n  0.4890299439430237,\n  0.5024400949478149,\n  0.489959716796875,\n  0.5239015221595764,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan],\n 'val_MCRMSE': [0.6079265475273132,\n  0.5815712809562683,\n  0.5743041634559631,\n  0.5780138969421387,\n  0.5671018958091736,\n  0.5611640214920044,\n  0.5686006546020508,\n  0.5547209978103638,\n  0.6367314457893372,\n  0.5786653757095337,\n  0.5073511600494385,\n  0.5890087485313416,\n  0.5128040909767151,\n  0.5907643437385559,\n  0.5138944387435913,\n  0.5111724138259888,\n  0.5109239220619202,\n  0.5327895283699036,\n  0.526545524597168,\n  0.5237370729446411,\n  0.4997272193431854,\n  0.5064727663993835,\n  0.4894353449344635,\n  0.4882499873638153,\n  0.4945613443851471,\n  0.4915541410446167,\n  0.48917245864868164,\n  0.5527468919754028,\n  0.5490846633911133,\n  0.5099660754203796,\n  0.5395488739013672,\n  0.5033068656921387,\n  0.4855077266693115,\n  0.5752233862876892,\n  0.48343825340270996,\n  0.5576025247573853,\n  0.5290079712867737,\n  0.5675570964813232,\n  0.48121678829193115,\n  0.4808445870876312,\n  0.4927208125591278,\n  0.5641881823539734,\n  0.4852254092693329,\n  0.5041536092758179,\n  0.48778262734413147,\n  0.5285024642944336,\n  0.48306646943092346,\n  0.4902433753013611,\n  0.5033167600631714,\n  0.4910886287689209,\n  0.5245670080184937,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan,\n  nan]}"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[nan]], dtype=float32)>"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tf.expand_dims(tf.convert_to_tensor(train_df.iloc[0][['tokens', 'pos']]), 0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
